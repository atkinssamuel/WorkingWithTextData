{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Demonstrating Python modules and data structures that can be used to efficiently work with Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python modules can be used to efficiently work with Twitter data:\n",
    "\n",
    "tweepy (this module will be used in this notebook for our demonstration of the Twitter API for Python)\n",
    "\n",
    "Python Twitter Tools\n",
    "\n",
    "python-twitter\n",
    "\n",
    "twython\n",
    "\n",
    "TwitterAPI\n",
    "\n",
    "TwitterSearch\n",
    "\n",
    "Source: https://stackabuse.com/accessing-the-twitter-api-with-python/\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structures used in the Python API for Twitter\n",
    "    \n",
    "Typically, Twitter data is pulled using the JSON data structure which you would have to parse either into csv or a pandas dataframe, depending on your purpose of the results.\n",
    "\n",
    "In the module used in this demonstration (Tweepy), tweets are pulled as tweepy objects. These objects are then converted into json so that we can parse through keys and values easier to gather tweet metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Using the Twitter API for Python to download tweets, search tweets by hashtags, extract metadata (i.e. number of reteweets, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary libraries used for Twitter API extraction and analysis are tweepy, csv, and json. Tweepy is the Twitter API library for Python, which is the most mature compared to all python libraries available for the Twitter API. The CSV library is used to save extracted tweets and underlying metadata into. The JSON library is used to parse and format tweet metadata into a format which is easy to manipulate because we can use dict keys and values to extract underlying metadata details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import datetime\n",
    "import csv\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessor as p\n",
    "#!pip install gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import re\n",
    "#!pip install wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State the keys to authenticate to the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to setup and be approved for a Developer account in order to receive these keys. These access keys are necessary in order to authenticate into the Twitter API using the tweepy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'crdecmmwhUaTV7oitShaB7xlV'\n",
    "consumer_secret= 'pEE16H07j9ygOmaxPyJBlW9LUZIrkjOwSyBwhk3DWTS5yZKzEX'\n",
    "access_token= '1242649299978256389-Ba9M1Nudxuue16nFtGAXuzPk5NNnja'\n",
    "access_token_secret= 'Skqz04ZBTGAob4K61cHBSay3myFyGLJiCUFPjd7rxyEIk'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate to your Twitter App  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you receive your access keys from your Twitter Developer account, pass the access key values into the OAuth handler, which is a function of the tweepy library that allows us to authenticate given acceptable credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User(_api=<tweepy.api.API object at 0x0000027864FD49E8>, _json={'id': 1242649299978256389, 'id_str': '1242649299978256389', 'name': 'Rohith', 'screen_name': 'rohith_so', 'location': 'Toronto, Canada', 'profile_location': None, 'description': 'PhD researcher in Machine Learning @UofT and Cloud security Engineer @Deloitte', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': True, 'followers_count': 3, 'friends_count': 63, 'listed_count': 0, 'created_at': 'Wed Mar 25 03:07:47 +0000 2020', 'favourites_count': 5, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 0, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1257200110250921984/a2rubQEa_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1257200110250921984/a2rubQEa_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1242649299978256389/1588575155', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none', 'suspended': False, 'needs_phone_verification': False}, id=1242649299978256389, id_str='1242649299978256389', name='Rohith', screen_name='rohith_so', location='Toronto, Canada', profile_location=None, description='PhD researcher in Machine Learning @UofT and Cloud security Engineer @Deloitte', url=None, entities={'description': {'urls': []}}, protected=True, followers_count=3, friends_count=63, listed_count=0, created_at=datetime.datetime(2020, 3, 25, 3, 7, 47), favourites_count=5, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=0, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='F5F8FA', profile_background_image_url=None, profile_background_image_url_https=None, profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1257200110250921984/a2rubQEa_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1257200110250921984/a2rubQEa_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/1242649299978256389/1588575155', profile_link_color='1DA1F2', profile_sidebar_border_color='C0DEED', profile_sidebar_fill_color='DDEEF6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=False, default_profile=True, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none', suspended=False, needs_phone_verification=False)\n"
     ]
    }
   ],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "     \n",
    "users = api.me()\n",
    "print(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search through tweets by hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search for the hashtag #wildfires which we define as a search term. We then use Tweepy's Cursor function to pass this search term into Tweepy's api.search function which allows us to conduct queries on available public tweets from a specified date, which is defined by the variable \"date_since\". We defined this variable to extract data from November 16th 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.cursor.ItemIterator object at 0x00000278656BC4E0>\n",
      "1323643751483416578 RT @ForestServiceNW: The #wildfires around the Pacific NW may be contained, but there will be years' worth of work to be done to repair inf‚Ä¶\n",
      "1323643608269029376 RT @jramireztsyl: I just ran the numbers for today's #RSFire2020. To date, we have done 30.6 billion simulations of virtual #wildfires in #‚Ä¶\n",
      "1323643548583878657 With #wildfires burning across California, nearly 100,000 people have been forced to #evacuate their homes &amp; almost‚Ä¶ https://t.co/apRL9pCNkl\n",
      "1323642832180154368 9 tips for #familyCaregivers to prepare #seniors for #wildfires. #caregiving #eldercare #aging #HomeInstead https://t.co/giDjPOU57b\n",
      "1323642439928848385 9 tips for #familyCaregivers to prepare #olderAdults for #wildfires. #caregiving #seniorcare #aging #HomeInstead https://t.co/yls8718zvG\n"
     ]
    }
   ],
   "source": [
    "# Define the search term and the date_since date as variables\n",
    "search_hashtag = \"#wildfires\"\n",
    "date_since = \"2018-11-16\"\n",
    "\n",
    "# Collect tweets\n",
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_hashtag,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(5)\n",
    "print(tweets)\n",
    "\n",
    "# Iterate and print tweets\n",
    "for tweet in tweets:\n",
    "    print(tweet.id, tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract metadata (i.e. number of retweets etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function api.get_status to pull the full text of a retweeted status given a tweet ID then convert this object into JSON format in order to manipulate the underlying metadata elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a coding fanatic who wants to work with us and learn new technologies? üë®‚Äçüíªüë©‚Äçüíª\n",
      "Well then, we are looking just for you!\n",
      "\n",
      "Register for our SDE Hiring Challenge right now!\n",
      "https://t.co/Zg08gHhT0W  \n",
      "\n",
      "#hiring #challenge #coding #programming https://t.co/1N7gXaH9eA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Thu May 28 06:14:48 +0000 2020',\n",
       " 'id': 1265889240300257280,\n",
       " 'id_str': '1265889240300257280',\n",
       " 'full_text': 'Are you a coding fanatic who wants to work with us and learn new technologies? üë®\\u200düíªüë©\\u200düíª\\nWell then, we are looking just for you!\\n\\nRegister for our SDE Hiring Challenge right now!\\nhttps://t.co/Zg08gHhT0W  \\n\\n#hiring #challenge #coding #programming https://t.co/1N7gXaH9eA',\n",
       " 'truncated': False,\n",
       " 'display_text_range': [0, 242],\n",
       " 'entities': {'hashtags': [{'text': 'hiring', 'indices': [203, 210]},\n",
       "   {'text': 'challenge', 'indices': [211, 221]},\n",
       "   {'text': 'coding', 'indices': [222, 229]},\n",
       "   {'text': 'programming', 'indices': [230, 242]}],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'urls': [{'url': 'https://t.co/Zg08gHhT0W',\n",
       "    'expanded_url': 'https://practice.geeksforgeeks.org/contest/hiring-challenge-sde',\n",
       "    'display_url': 'practice.geeksforgeeks.org/contest/hiring‚Ä¶',\n",
       "    'indices': [176, 199]}],\n",
       "  'media': [{'id': 1265887151016812546,\n",
       "    'id_str': '1265887151016812546',\n",
       "    'indices': [243, 266],\n",
       "    'media_url': 'http://pbs.twimg.com/media/EZFVqCoWoAILfq5.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/EZFVqCoWoAILfq5.jpg',\n",
       "    'url': 'https://t.co/1N7gXaH9eA',\n",
       "    'display_url': 'pic.twitter.com/1N7gXaH9eA',\n",
       "    'expanded_url': 'https://twitter.com/geeksforgeeks/status/1265889240300257280/photo/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'medium': {'w': 1200, 'h': 1200, 'resize': 'fit'},\n",
       "     'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1200, 'h': 1200, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 680, 'resize': 'fit'}}}]},\n",
       " 'extended_entities': {'media': [{'id': 1265887151016812546,\n",
       "    'id_str': '1265887151016812546',\n",
       "    'indices': [243, 266],\n",
       "    'media_url': 'http://pbs.twimg.com/media/EZFVqCoWoAILfq5.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/EZFVqCoWoAILfq5.jpg',\n",
       "    'url': 'https://t.co/1N7gXaH9eA',\n",
       "    'display_url': 'pic.twitter.com/1N7gXaH9eA',\n",
       "    'expanded_url': 'https://twitter.com/geeksforgeeks/status/1265889240300257280/photo/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'medium': {'w': 1200, 'h': 1200, 'resize': 'fit'},\n",
       "     'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1200, 'h': 1200, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 680, 'resize': 'fit'}}}]},\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'user': {'id': 57741058,\n",
       "  'id_str': '57741058',\n",
       "  'name': 'GeeksforGeeks',\n",
       "  'screen_name': 'geeksforgeeks',\n",
       "  'location': 'India',\n",
       "  'description': 'üë®üèª\\u200düíªüßëüèº\\u200düíªüë©üèª\\u200düíª - üëçüèª ; üêùüêûüêõüêåüêúüï∑ü¶óü¶ü - ‚ùå',\n",
       "  'url': 'https://t.co/1Dm8vpxhFQ',\n",
       "  'entities': {'url': {'urls': [{'url': 'https://t.co/1Dm8vpxhFQ',\n",
       "      'expanded_url': 'http://geeksforgeeks.org',\n",
       "      'display_url': 'geeksforgeeks.org',\n",
       "      'indices': [0, 23]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 20790,\n",
       "  'friends_count': 22,\n",
       "  'listed_count': 156,\n",
       "  'created_at': 'Fri Jul 17 20:02:09 +0000 2009',\n",
       "  'favourites_count': 712,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': False,\n",
       "  'statuses_count': 13907,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'FFFDF7',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme13/bg.gif',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme13/bg.gif',\n",
       "  'profile_background_tile': False,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1304985167476523008/QNHrwL2q_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1304985167476523008/QNHrwL2q_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/57741058/1603173723',\n",
       "  'profile_link_color': '119E39',\n",
       "  'profile_sidebar_border_color': 'D3D2CF',\n",
       "  'profile_sidebar_fill_color': 'E3E2DE',\n",
       "  'profile_text_color': '0D0C0C',\n",
       "  'profile_use_background_image': False,\n",
       "  'has_extended_profile': False,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'none'},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 7,\n",
       " 'favorite_count': 18,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'possibly_sensitive': False,\n",
       " 'possibly_sensitive_appealable': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the full text of a retweeted status of a given tweet ID by first checking if the tweet has been retweeted\n",
    "\n",
    "id = \"1265889240300257280\"\n",
    "status = api.get_status(id, tweet_mode=\"extended\")\n",
    "try:\n",
    "    print(status.retweeted_status.full_text)\n",
    "except AttributeError: # Not a Retweet\n",
    "    print(status.full_text)\n",
    "\n",
    "#Convert the tweet status into JSON so we can parse the dict keys and gather underlying metadata\n",
    "json_str = json.dumps(status._json)\n",
    "metadata = (json.loads(json_str))\n",
    "metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all elements of the metadata variable (JSON format of the retweeted status object) can be seen in a clean JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the metadata variable is now in JSON format, we can view the keys of the variable becuase it is a dictionary data structure in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', 'extended_entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'possibly_sensitive_appealable', 'lang'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gather the keys of the tweet's metadata\n",
    "metadata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these key values, we can now easily identify how we want to extract underlying metadata elements by searching through the keys of the metadata dict variable. For example, as seen below we can get the user metadata information by analyzing the name key within the user key.\n",
    "\n",
    "We now use this to gather when the tweet was published, by which user, from wht country, and how many followeres and friends the user has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet was created at Thu May 28 06:14:48 +0000 2020 by the user GeeksforGeeks from India \n",
      "This user has 20790 followers and 22 friends\n"
     ]
    }
   ],
   "source": [
    "#Gather the user of the tweet\n",
    "user = metadata['user']['name']\n",
    "#Gather the location of a user's tweet\n",
    "user_location = metadata['user']['location']\n",
    "\n",
    "#Gather the time the tweet was made\n",
    "created_at = metadata['created_at']\n",
    "\n",
    "#Gather details about the user's followers and friends\n",
    "number_of_followers = metadata['user']['followers_count']\n",
    "number_of_friends = metadata['user']['friends_count']\n",
    "\n",
    "print(\"The tweet was created at\",created_at,\"by the user\",user,\"from\",user_location,\"\\nThis user has\",number_of_followers,\"followers and\",number_of_friends,\"friends\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the Tweet ID, there were: 7 retweets found\n"
     ]
    }
   ],
   "source": [
    "#printing the number of retweets for a tweet \n",
    "retweets_list = api.retweets(id) \n",
    "\n",
    "number_of_retweets = len(retweets_list)\n",
    "print(\"\\nBased on the Tweet ID, there were:\", number_of_retweets, \"retweets found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Using the Twitter API to download tweets and save those as a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we search for the last 100 tweets made using the hashtag \"#trump\" and save these tweets to a csv along with metadata of those tweets: username/screen name, id of the tweet, whether it was retweeted, language of the tweet, number of followers of the user, whether the user is verified, location the tweet was made in, the tweet, and when it was created. Our search results are then saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets have been saved to the following csv file:hashtag_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "#method to get a user's last tweets\n",
    "def get_tweets(hashtag):\n",
    "\n",
    " \n",
    "\n",
    "    #set count to however many tweets you want\n",
    "    number_of_tweets = 100\n",
    "\n",
    "    #get tweets\n",
    "    tweets_for_csv = []\n",
    "    for tweet in tw.Cursor(api.search, q = hashtag).items(number_of_tweets):\n",
    "        #create array of tweet information: username, tweet id, date/time, text\n",
    "        tweets_for_csv.append([tweet.user.screen_name,tweet.retweeted,tweet.user.lang,tweet.user.followers_count,tweet.user.verified,tweet.user.location.encode(\"utf-8\"),tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")])\n",
    "\n",
    "    #write to a new csv file from the array of tweets\n",
    "    outfile = \"hashtag_tweets.csv\"\n",
    "    print (\"tweets have been saved to the following csv file:\" + outfile)\n",
    "    with open(outfile, 'w+') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerows(tweets_for_csv)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_tweets(\"#trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Basic feature extraction and basic text preprocessing on tweets from csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted tweets from Twitter's API using the Tweepy python library, we can analyze and extract features from the subsequent csv file we've dumped our data into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the nltk Python library has a function called 'stopwords' which allows us to remove all stopwords in the text of each tweet. Stop words do not help us to find the context or the true meaning of a text phrase. These are words that can be removed without affecting the machine learning model that you would train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rsothilingam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>retweeted_or_not</th>\n",
       "      <th>number_Tweets_issued</th>\n",
       "      <th>num_followers</th>\n",
       "      <th>verified_or_not</th>\n",
       "      <th>location_the_tweet</th>\n",
       "      <th>when_the_tweet_created</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4_kwt1</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34656</td>\n",
       "      <td>False</td>\n",
       "      <td>b'\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xaf\\xd9\\x8a\\xd9\\x84\\xd9\\x8a\\xd9\\x87, \\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd9\\x88\\xd9\\x8a\\xd8\\xaa'</td>\n",
       "      <td>1323649600616566789</td>\n",
       "      <td>2020-11-03 15:33:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ma_cologne</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73</td>\n",
       "      <td>False</td>\n",
       "      <td>b''</td>\n",
       "      <td>1323649599203086336</td>\n",
       "      <td>2020-11-03 15:33:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deepak57510795</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>False</td>\n",
       "      <td>b''</td>\n",
       "      <td>1323649592613773312</td>\n",
       "      <td>2020-11-03 15:33:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NewYorkerinLACA</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294</td>\n",
       "      <td>False</td>\n",
       "      <td>b'The Kingdom of Iran'</td>\n",
       "      <td>1323649589744816128</td>\n",
       "      <td>2020-11-03 15:33:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_name  retweeted_or_not  number_Tweets_issued  num_followers  \\\n",
       "5           4_kwt1             False                   NaN          34656   \n",
       "6       ma_cologne             False                   NaN             73   \n",
       "7   Deepak57510795             False                   NaN             69   \n",
       "8  NewYorkerinLACA             False                   NaN            294   \n",
       "\n",
       "   verified_or_not  \\\n",
       "5            False   \n",
       "6            False   \n",
       "7            False   \n",
       "8            False   \n",
       "\n",
       "                                                                                                                                       location_the_tweet  \\\n",
       "5  b'\\xd8\\xa7\\xd9\\x84\\xd8\\xb9\\xd8\\xaf\\xd9\\x8a\\xd9\\x84\\xd9\\x8a\\xd9\\x87, \\xd8\\xaf\\xd9\\x88\\xd9\\x84\\xd8\\xa9 \\xd8\\xa7\\xd9\\x84\\xd9\\x83\\xd9\\x88\\xd9\\x8a\\xd8\\xaa'   \n",
       "6                                                                                                                                                     b''   \n",
       "7                                                                                                                                                     b''   \n",
       "8                                                                                                                                  b'The Kingdom of Iran'   \n",
       "\n",
       "   when_the_tweet_created                 text  \n",
       "5     1323649600616566789  2020-11-03 15:33:51  \n",
       "6     1323649599203086336  2020-11-03 15:33:51  \n",
       "7     1323649592613773312  2020-11-03 15:33:49  \n",
       "8     1323649589744816128  2020-11-03 15:33:49  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"hashtag_tweets.csv\",index_col=False, names=[\"user_name\",\"retweeted_or_not\", \"number_Tweets_issued\", \"num_followers\", \"verified_or_not\", \"location_the_tweet\", \"when_the_tweet_created\", \"text\" ]) \n",
    "df.iloc[5:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df[['text']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet text  Preprocessing \n",
    "Since we are dealing with tweets, we need to do specific tweet text cleaning along with normal text pre-processing. A tweet may contains\n",
    "\n",
    "- URL's\n",
    "- Mentions\n",
    "- Hashtags\n",
    "- Emojis\n",
    "- Smileys\n",
    "- Spefic words etc..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean the tweet , we can use a python library tweet-preprocessor instead of writing the cleaning logic ourself. Tweet-preprocessor is a preprocessing library for tweet data written in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install tweet-preprocessor \n",
    "!pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clean the text\n",
    "def preprocess_tweet(row):\n",
    "    text = row['text']\n",
    "    text = p.clean(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text_embed_Preprocessor'] = df1.apply(preprocess_tweet, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tweet has been cleaned to normal text.\n",
    "df1.loc[6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''banned = [u\"\\U0001F600-\\U0001F64F\".encode(\"utf-8\"), \n",
    "          u\"\\U0001F300-\\U0001F5FF\".encode(\"utf-8\"),  # symbols & pictographs\n",
    "          u\"\\U0001F680-\\U0001F6FF\".encode(\"utf-8\"), # transport & map symbols\n",
    "          u\"\\U0001F1E0-\\U0001F1FF\".encode(\"utf-8\"), # flags (iOS)\n",
    "         ]\n",
    "df[\"Text\"].apply(lambda x: [item for item in x if item not in banned])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can apply normal text preprocessing\n",
    "\n",
    "For example, we will apply the following types of pre-processing to our data now that we have taken the previous step of using the tweet-preprocessor library:\n",
    "- Lowercasing\n",
    "- Punctuation Removal\n",
    "- Replace extra white spaces\n",
    "- Stopwords removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below will remove stop words from a given row of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopword_removal(row):\n",
    "    text = row['text']\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply this function to the string text values after the previous pre-processing step of using tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text_after_preprocess'] = df1.apply(stopword_removal, axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove extra white spaces, punctuation and apply lower casing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets require further pre-processing steps such as extra white spaces in message strings as well as emojis which users may include in their tweets. Here, we remove these unnecessary elements of the tweet message strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text_after_preprocess'] = df1['text_after_preprocess'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')\n",
    "df1.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Emoji from the words (remove all words which contains number)\n",
    "\n",
    "e=[]\n",
    "for i in range(0,100):\n",
    "    string=df1['text_after_preprocess'].loc[i]\n",
    "    e.append(re.sub(r'\\w*\\d\\w*', '', string).strip())\n",
    "\n",
    "#create new df \n",
    "df3 = pd.DataFrame({'text':e})\n",
    "#add column to existing df \n",
    "df1['text_after_preprocess'] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#remove Emoji from the words (remove all words which contains number)\n",
    "import re\n",
    "e=[]\n",
    "for i in range(0,100):\n",
    "    string=df['text'].loc[i]\n",
    "    e.append(re.sub(r'\\w*\\d\\w*', '', string).strip())\n",
    "\n",
    "#create new df \n",
    "df1 = pd.DataFrame({'text':e})\n",
    "#add column to existing df \n",
    "df['text'] = e'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text_after_preprocess'] = df1['text_after_preprocess'].str[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text_after_preprocess'] =df1['text_after_preprocess'].str.replace(r'xba', '')\n",
    "df1['text_after_preprocess']=df1['text_after_preprocess'].str.replace(r'xbb', '')\n",
    "df1['text_after_preprocess'] =df1['text_after_preprocess'].str.replace(r'xac', '')\n",
    "df1['text_after_preprocess'] =df1['text_after_preprocess'].str.replace(r'xef', '')\n",
    "df1['text_after_preprocess'] =df1['text_after_preprocess'].str.replace(r'xbf', '')\n",
    "df1['text_after_preprocess'] =df1['text_after_preprocess'].str.replace(r'xbd', '')\n",
    "df1['text_after_preprocess'] =df1['text_after_preprocess'].str.replace(r'xbe', '')\n",
    "df1['text_after_preprocess']=df1['text_after_preprocess'].str.replace(r'rt', '')\n",
    "df1.loc[4:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully completed pre-processing for our data, we can pursue basic feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "df[['text','word_count']].iloc[5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code extracts features and analyzes them using the Python wordcloud library. This will allow us toa nalyze the types of strings associated with the tweet \"#trump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start with one review:\n",
    "df_ADR = df[df['text']==1]\n",
    "df_NADR = df[df['text']==0]\n",
    "tweet_All = \" \".join(review for review in df.text)\n",
    "tweet_ADR = \" \".join(review for review in df_ADR.text)\n",
    "tweet_NADR = \" \".join(review for review in df_NADR.text)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize  = (10,30))\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_All)\n",
    "#wordcloud_ADR = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_ADR)\n",
    "#wordcloud_NADR = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_NADR)\n",
    "\n",
    "# Display the generated image:\n",
    "ax[0].imshow(wordcloud_ALL, interpolation='bilinear')\n",
    "ax[0].set_title('All Tweets', fontsize=30)\n",
    "ax[0].axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a tokenization function to tokenize each individual word in each row of tweet message strings. We can then extract further features based on words identified which are statistically significant to our target dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "df['Tweet_tokenized'] = df['text'].apply(lambda x: tokenization(x.lower()))\n",
    "df1=df[['text','word_count','Tweet_tokenized']]\n",
    "df1.iloc[5:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the CountVectorizer function of the sklearn library to vectorize. by using a vectorized implementation in an optimization algorithm we can make the process of computation much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of countvectorizer\n",
    "vect = CountVectorizer()  # shift tab "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
