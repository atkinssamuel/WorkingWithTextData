{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit (conda)",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "cf5781ee9688be887a830af3862a813ef5b0a355a8789de10c6994358407057c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# importing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import nltk\n",
    "import torch\n",
    "import torchtext\n",
    "import gensim"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 52,
   "outputs": []
  },
  {
   "source": [
    "# Document Feature Extraction, Text Processing, and Word Embedding\n",
    "## Requirements\n",
    "- Introduce scikit-learn, nltk, and other text processing libraries\n",
    "- Explain basic feature extraction from text data\n",
    "    - Number of words\n",
    "    - Number of characters\n",
    "    - Average word length\n",
    "    - Number of stopwords\n",
    "    - Other feature extraction techniques that may be relevant\n",
    "- Explan advanced text processing\n",
    "    - N-grams\n",
    "    - Term Frequency (TF)\n",
    "    - Inverse Document Frequency (IDF)\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    - \"Bag of Words\" document representation\n",
    "    - Word embedding\n",
    "    - Other text processing methods that may be important"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Document Feature Extraction\n",
    "## TF/IDF\n",
    "Term Frequency (TF) and Inverse Document Frequency (IDF) are useful data that will inform us as to which terms are the most relevant to a given document in a corpus. TF simply measures the frequency of a given word in a document. This measurement is useful because typically, common words are relevant to the meaning of that document. TF can be calculated in the following way:\n",
    "\n",
    "$TF = OC / T$\n",
    "\n",
    "$OC$ is the total number of occurences of that particular word in the document and $T$ is the total number of words in the document. Suppose we have access to all of the articles on Wikipedia. Our corpus, in this context, is all of these Wikipedia articles combined. Each of these articles would be its own entity called a document. Document Frequency (DF) is a measure of how frequently a given word appears in the entire corpus of text. The IDF is the inverst of the DF:\n",
    "\n",
    "$IDF = \\frac{1}{DF}$\n",
    "\n",
    "Using the TF and the IDF we can measure how important and unique a given word is for a particular document. Since word frequencies are distributed exponentially, we use the log of the IDF to obtain a measure for the value of a word in a given document:\n",
    "\n",
    "Word Value (WV) $= TF \\cdot log(IDF)$\n",
    "\n",
    "This approach treats the documents in a corpus as a \"bag of words\". This means that this approach does not take into account the meaning of sentences or the contexts in which these words are used in. This restricts the applications of this method severely. \n",
    "\n",
    "### TF/IDF Application\n",
    "Suppose we wish to search our Wikipedia corpus for a document most relevant to Serena Williams. We could compute the WV for \"Serena Williams\" accross all of the documents. Then, we could the set of documents that had the highest \"Serena Williams\" WVs. This would be a reasonably effective way to query a corpus for relevant documents. \n",
    "\n",
    "# N-Gram Models\n",
    "N-Gram models are very useful models in the field of text analysis. An N-Gram model can predict the probability of a word occurring based on the occurence of its N-1 words. These models have a tremendous amount of use cases. They can be used to determine which word belongs in a particular sentence for the purpose of text generation. They can be used to detect spelling errors in sentences. They can also be used as speech recognition engines. \n",
    "\n",
    "We will illustrate a simple example of how an N-Gram model can be used to detect a spelling error. Suppose we are given the following set of sentences:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Today I went to the store. Yesterday it took me 10 minutes to drive there. Today, it took me 15 minutes to get there and 15 minuets to get back.\""
   ]
  },
  {
   "source": [
    "\"minutes\" was spelled as \"minuets\" in the third sentence. If we implement a simple bi-gram (2-gram) model on this corpus, we can compute the probability of each word appearing as a function of the word appearing immediately before that word. The following snippet tokenizes the above sentence and computes the bi-gram frequencies:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(to, get)              2\n(minute, to)           2\n(took, me)             2\n(it, took)             2\n(drive, there.)        1\n(there., Today)        1\n(back, .)              1\n(there, and)           1\n(I, went)              1\n(and, 15)              1\n(Today, I)             1\n(Today, ,)             1\n(Yesterday, it)        1\n(went, to)             1\n(store., Yesterday)    1\n(me, 10)               1\n(15, minute)           1\n(minuet, to)           1\n(get, back)            1\n(get, there)           1\n(the, store.)          1\n(10, minute)           1\n(,, it)                1\n(15, minuet)           1\n(to, the)              1\n(me, 15)               1\n(to, drive)            1\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "tree_bank_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = tree_bank_tokenizer.tokenize(sentence)\n",
    "words = [wnlemmatizer.lemmatize(word) for word in words]\n",
    "n_grams_series = pd.Series(nltk.ngrams(words, 2))\n",
    "print(n_grams_series.value_counts())"
   ]
  },
  {
   "source": [
    "From the snippet above, we can see that the word \"minute\" appears twice before the word \"to\". In the original corpus, the word \"to\" appears 3 times, not including the time when it appeared after \"minuets\". Therefore, there is a probability of 2/3 that \"minuets\" should be \"minutes\" and a probability of 1/3 that it should be \"went\".\n",
    "\n",
    "In this simple example a bi-gram model was able to detect a spelling mistake. By using a large corpus we can achieve very effective N-gram models. Further, increasing the model's \"N\" we can sometimes increase the effectiveness of the model. Increasing \"N\" can also, however, harm the effectiveness of the model by using irrelevant words to determine the context of a given word. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Text Processing for Complex Models\n",
    "## Why is Text Processing Important?\n",
    "Natural language processing is a branch of artificial intelligence in which unstructured text data is broken down into information that can be analyzed by models. Especially in the age of social media, humans commuincate online through unstructured text messages all the time. Whether it be through Twitter, private messaging platforms like Facebook Messenger, or through product reviewing interfaces like Yelp. The ability to transform these unstructured blobs of text into forms that are machines can understand opens the door to a wide variety of applications. \n",
    "\n",
    "These applications primarily fall under text classification and generative models. Text classification agents try to uncover the intent, logic, and/or meaning behind blobs of text. This model type can be implemented in all sorts of contexts. For example, as an AI that can automatically sort through text reviews to characterize the performance of a fast-food chain. \n",
    "\n",
    "Processed text can also be used as an input to generative models. These generative models learn how to commuincate by training on large blobs of processed text. These models can be used to generate speeches or converse with customers, for example.\n",
    "\n",
    "## Tokenization\n",
    "When trying to analyze a corpus, some characters, a chain of words, or a set of reviews, we are initially provided with some text. This text may contain spelling errors, they may or may not obey the grammatical rules of the English language, and they may contain punctuation. For our analysis, we will consider Amazon review data. The following code snippets load the Amazon review dataset and display a handful of example reviews:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "# available: https://www.kaggle.com/bittlingmayer/amazonreviews\n",
    "train_data_file = bz2.BZ2File(\"amazon-review-data/train.ft.txt.bz2\")\n",
    "test_data_file = bz2.BZ2File(\"amazon-review-data/test.ft.txt.bz2\")\n",
    "train_lines = train_data_file.readlines()\n",
    "test_lines = test_data_file.readlines()\n",
    "del train_data_file, test_data_file\n",
    "train_lines = [x.decode('utf-8') for x in train_lines]\n",
    "test_lines = [x.decode('utf-8') for x in test_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Lines Length = 3600000\nTest Lines Length = 400000\nLength of Total Dataset = 4000000\nPercentage of Training Data = 90.0%\nPercentage of Testing Data = 10.0%\n"
     ]
    }
   ],
   "source": [
    "# dataset properties\n",
    "train_lines_len = len(train_lines)\n",
    "test_lines_len = len(test_lines)\n",
    "total_length = train_lines_len + test_lines_len\n",
    "print(\"Train Lines Length =\", train_lines_len)\n",
    "print(\"Test Lines Length =\", test_lines_len)\n",
    "\n",
    "total_length = train_lines_len + test_lines_len\n",
    "\n",
    "print(\"Length of Total Dataset =\", total_length)\n",
    "print(\"Percentage of Training Data = {}%\".format(round(train_lines_len / total_length * 100, 3)))\n",
    "print(\"Percentage of Testing Data = {}%\".format(round(test_lines_len / total_length * 100, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Amazon Example Review at Index 99\nCaution!: These tracks are not the \"original\" versions but are re-recorded versions. So, whether the tracks are \"remastered\" or not is irrelevant.\n\nAmazon Example Review at Index 90\nNo instructions included - do not trust seller: Promised with this item are \"Complete Instructions\" and the additional pledge that \"Sweet Graces will email you with the Supply List and Instruction sheets on purchase - so you can be ready ahead of time!\" I received none of this - only a plastic figurine and bracelet. To boot, Amazon claims they can do nothing to help me contact the seller. All I got was a phone number for the manufacturer. Let's hope that yields some results. Meanwhile, I'm wishing I had listened to previous feedback about this unreliable seller :/\n\nAmazon Example Review at Index 15\nDon't try to fool us with fake reviews.: It's glaringly obvious that all of the glowing reviews have been written by the same person, perhaps the author herself. They all have the same misspellings and poor sentence structure that is featured in the book. Who made Veronica Haddon think she is an author?\n\n"
     ]
    }
   ],
   "source": [
    "# amazon review examples\n",
    "np.random.seed(20)\n",
    "random_indices = [np.random.randint(100) for x in range(3)]\n",
    "for index in random_indices:\n",
    "    print(\"Amazon Example Review at Index {}\".format(index))\n",
    "    print(train_lines[index][11:])"
   ]
  },
  {
   "source": [
    "As shown above, these text reviews contain a few sentences strung together to form a review. These sentences contain punctuation, capital letters, slang, and spelling mistakes. Our goal when processing these blobs of text is to take these unstructured sentences and transform them into a set of tokens that a model can understand. Currently, the model cannot understand the meaning of these sentences.\n",
    "\n",
    "There are several natural language processing libraries in Python that allow us to create tokens from these unstructured sentences. The way in which we break these sentences apart will determine the effectiveness of the model.\n",
    "\n",
    "To create tokens, we must first consider the way in which we understand the English language. A sentence is made up of words. These words are separated by spaces. We can tokenize a sentence by splitting up the sentence using white-space as a delimiter. Consider the following text entry:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i liked this album more then i thought i would: I heard a song or two and thought same o same o,but when i listened to songs like \"blue angel\",\"lanna\" and 'mama\" the hair just rose off my neck.Roy is trully an amazing singer with a talent you don't find much now days.\n\n"
     ]
    }
   ],
   "source": [
    "text_entry_example = train_lines[24][11:]\n",
    "print(text_entry_example)"
   ]
  },
  {
   "source": [
    "### WhitespaceTokenizer\n",
    "Using the ```nltk.tokenize.WhitespaceTokenizer```, we can separate the above review using white-space as the delimiter:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "white_space_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "print(white_space_tokenizer.tokenize(text_entry_example))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'liked', 'this', 'album', 'more', 'then', 'i', 'thought', 'i', 'would:', 'I', 'heard', 'a', 'song', 'or', 'two', 'and', 'thought', 'same', 'o', 'same', 'o,but', 'when', 'i', 'listened', 'to', 'songs', 'like', '\"blue', 'angel\",\"lanna\"', 'and', '\\'mama\"', 'the', 'hair', 'just', 'rose', 'off', 'my', 'neck.Roy', 'is', 'trully', 'an', 'amazing', 'singer', 'with', 'a', 'talent', 'you', \"don't\", 'find', 'much', 'now', 'days.']\n"
     ]
    }
   ]
  },
  {
   "source": [
    "The problem with this tokenization method is that tokens like 'would:', 'neck.Roy', '\"blue', 'angel\"', and 'days.' don't have much meaning. 'would:' and 'would' have different meanings. Further, 'neck.Roy' and 'neck', 'roy' also have different meanings. We need to somehow take into account the puntuation present in each review. Further, we need to factor in capital letters, plural and singular forms of words, and spelling mistakes. Consider another example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Don't try to fool us with fake reviews.: It's glaringly obvious that all of the glowing reviews have been written by the same person, perhaps the author herself. They all have the same misspellings and poor sentence structure that is featured in the book. Who made Veronica Haddon think she is an author?\n\n"
     ]
    }
   ],
   "source": [
    "text_entry_example = train_lines[15][11:]\n",
    "print(text_entry_example)"
   ]
  },
  {
   "source": [
    "### WordPunctTokenizer\n",
    "To take into account the punctuation present in a review, we will try the ```nltk.tokenize.WordPunctTokenizer```:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Don', \"'\", 't', 'try', 'to', 'fool', 'us', 'with', 'fake', 'reviews', '.:', 'It', \"'\", 's', 'glaringly', 'obvious', 'that', 'all', 'of', 'the', 'glowing', 'reviews', 'have', 'been', 'written', 'by', 'the', 'same', 'person', ',', 'perhaps', 'the', 'author', 'herself', '.', 'They', 'all', 'have', 'the', 'same', 'misspellings', 'and', 'poor', 'sentence', 'structure', 'that', 'is', 'featured', 'in', 'the', 'book', '.', 'Who', 'made', 'Veronica', 'Haddon', 'think', 'she', 'is', 'an', 'author', '?']\n"
     ]
    }
   ],
   "source": [
    "word_punct_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(text_entry_example))"
   ]
  },
  {
   "source": [
    "From the above output we can see tokens like \"t\", \"s\", and \"'\" are occurring. These tokens have no meaning. \n",
    "\n",
    "### TreebankWordTokenizer\n",
    "To solve this problem we can use a more advanced tokenization method that transforms these types of tokens into tokens that are more meaningful called the ```nltk.tokenize.TreebankWordTokenizer```:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Do', \"n't\", 'try', 'to', 'fool', 'us', 'with', 'fake', 'reviews.', ':', 'It', \"'s\", 'glaringly', 'obvious', 'that', 'all', 'of', 'the', 'glowing', 'reviews', 'have', 'been', 'written', 'by', 'the', 'same', 'person', ',', 'perhaps', 'the', 'author', 'herself.', 'They', 'all', 'have', 'the', 'same', 'misspellings', 'and', 'poor', 'sentence', 'structure', 'that', 'is', 'featured', 'in', 'the', 'book.', 'Who', 'made', 'Veronica', 'Haddon', 'think', 'she', 'is', 'an', 'author', '?']\n"
     ]
    }
   ],
   "source": [
    "tree_bank_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "text_entry_tb_output = tree_bank_tokenizer.tokenize(text_entry_example)\n",
    "print(text_entry_tb_output)"
   ]
  },
  {
   "source": [
    "\"Do\" + \"n't\" converys the meaning of \"Don't\" better than \"Don\", \"'\", \"t\". The ```TreebankWordTokenizer``` presents the most effective way to extract meaning from these sentences. \n",
    "\n",
    "## Token Normalization\n",
    "Now that we have split our data into tokens, we must further parse these tokens. It may be the case that we want the same token for different forms of a given word. For example, we may want both \"pen\" and \"pens\" to be represented by the \"pen\" token. Moreover, we may want \"person\", \"people\", and \"persons\" to all be represented by \"person\". There are two ways in which we can concatenate these tokens: stemming and lematization.\n",
    "\n",
    "### Stemming\n",
    "Stemming is the process of removing and/or replacing the suffixes of words to obtain the root meaning of the word. This normalization method simply cuts off the suffixes of various words to obtain simplified and understandable tokens. Let's apply the ```nltk.stem.PorterStemmer``` normalization method to a list of abnormal and plural words:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Word = persons, Stemmed Word = person\n\nOriginal Word = Feet, Stemmed Word = feet\n\nOriginal Word = apples, Stemmed Word = appl\n\nOriginal Word = Trying, Stemmed Word = tri\n\nOriginal Word = fries, Stemmed Word = fri\n\nOriginal Word = geese, Stemmed Word = gees\n\nOriginal Word = women, Stemmed Word = women\n\n"
     ]
    }
   ],
   "source": [
    "pstemmer = nltk.stem.PorterStemmer()\n",
    "plural_words = [\"persons\", \"Feet\", \"apples\", \"Trying\", \"fries\", \"geese\", \"women\"]\n",
    "for word in plural_words:\n",
    "    print(\"Original Word = {}, Stemmed Word = {}\\n\".format(word, pstemmer.stem(word)))"
   ]
  },
  {
   "source": [
    "From the output above, we can see that the stemmer handles words like persons, apples, and fries correctly. Notice that the stemmer also modifies the casing of each word such that the words are lower case. The stemmer fails, however, to modify women, geese, and feet to their singular counterparts. The following normalization method addresses these situations. \n",
    "\n",
    "### Lemmatization\n",
    "A lemmatizer looks up the tokens using a database formed using vast amounts of text. This normalization technique can properly address abnormal plural words. The following is an implementation of the ```nltk.stem.WordNetLemmatizer```:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Word = persons, Stemmed Word = person\n",
      "\n",
      "Original Word = Feet, Stemmed Word = Feet\n",
      "\n",
      "Original Word = apples, Stemmed Word = apple\n",
      "\n",
      "Original Word = Trying, Stemmed Word = Trying\n",
      "\n",
      "Original Word = fries, Stemmed Word = fry\n",
      "\n",
      "Original Word = geese, Stemmed Word = goose\n",
      "\n",
      "Original Word = women, Stemmed Word = woman\n",
      "\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\purpl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "for word in plural_words:\n",
    "    print(\"Original Word = {}, Stemmed Word = {}\\n\".format(word, wnlemmatizer.lemmatize(word)))"
   ]
  },
  {
   "source": [
    "The problem with this normalization technique is that some words that have different meanings are reduced to the same lemma in niche circumstances. It is important to determine the context in which these tokenization methods are used so that we can make an informed decision as to which one will work better.\n",
    "\n",
    "# Token Representations\n",
    "Now that we have normalized the tokens appropriately, we must represent these tokens in a way that a machine can understand them. Models like neural networks and SVMs accept rational vector inputs. We need to transform these string tokens into real-numbered vectors that are related to the string tokens in some meaningful way.\n",
    "\n",
    "Given a corpus, the vocabulary of the corpus is all of the unique words extracted from the corpus through tokenization. We need to represent every word in the vocabulary with some real-numbered vector. \n",
    "\n",
    "There are a few ways that we can do this. We can manually create representations for tokens by one-hot encoding each token uniquely or by manually training a set of custom word embeddings. Alternatively, we can use pre-trained word embeddings like Word2Vec embeddings and GloVe embeddings.\n",
    "\n",
    "Consider the following tokenized and lemmatized Amazon review example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "np.random.seed(8)\n",
    "sentence = train_lines[np.random.randint(100)][11:]\n",
    "print(\"Original Sentence:\\n{}\".format(sentence))\n",
    "\n",
    "tree_bank_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = tree_bank_tokenizer.tokenize(sentence)\n",
    "words = [wnlemmatizer.lemmatize(word) for word in words]\n",
    "print(\"\\nTokenized & Lemmatized Sentence:\\n{}\".format(words))\n",
    "\n",
    "vocabulary = set(words)\n",
    "print(\"\\nVocabulary:\\n{}\".format(vocabulary))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Sentence:\nEven Mommy has fun with this one!: My four year old daughter loves everything Barbie and loves the Rapunzel movie. This game is tons of fun, even for a 42 year old. We love playing it together. We love decorating all the rooms and finding the gems. What even better is, she can play it alone and I get some me time!\n\n\nTokenized & Lemmatized Sentence:\n['Even', 'Mommy', 'ha', 'fun', 'with', 'this', 'one', '!', ':', 'My', 'four', 'year', 'old', 'daughter', 'love', 'everything', 'Barbie', 'and', 'love', 'the', 'Rapunzel', 'movie.', 'This', 'game', 'is', 'ton', 'of', 'fun', ',', 'even', 'for', 'a', '42', 'year', 'old.', 'We', 'love', 'playing', 'it', 'together.', 'We', 'love', 'decorating', 'all', 'the', 'room', 'and', 'finding', 'the', 'gems.', 'What', 'even', 'better', 'is', ',', 'she', 'can', 'play', 'it', 'alone', 'and', 'I', 'get', 'some', 'me', 'time', '!']\n\nVocabulary:\n{'this', 'a', 'some', 'We', 'old', '42', 'playing', 'time', 'year', 'ha', 'fun', 'the', ',', 'can', 'My', 'for', 'with', 'one', 'movie.', 'I', 'old.', '!', 'love', 'Rapunzel', 'game', 'together.', 'Mommy', 'gems.', 'What', 'it', 'decorating', 'get', 'finding', 'play', 'four', 'alone', 'daughter', 'is', 'ton', 'all', 'Barbie', 'everything', 'of', 'even', ':', 'room', 'better', 'This', 'she', 'and', 'me', 'Even'}\n"
     ]
    }
   ]
  },
  {
   "source": [
    "We will demonstrate the ways in which we can encode the parsed tokens to create meaning for a potential model.\n",
    "\n",
    "## One-Hot Encoding\n",
    "To represent the vocabulary of the above sentence using a one-hot encoding representation we simply define one-hot vectors for each of the unique words in the corpus (the tokenized sentence). \n",
    "\n",
    "There are a few problems with this method. Firstly, the dimensions of the vectors required to represent the tokens is proportional to the number of words in the vocabulary. This will inevitably slow down future models. Secondly, the relationship between the words in the vocabulary is not captured by this representation. How do we compare Queen = \\[0, 0, 0, 1\\] and King = \\[0, 1, 0, 0\\]? \n",
    "\n",
    "\n",
    "## GloVe Embeddings\n",
    "Instead of using one-hot encodings we can use pre-trained \"GloVe\" embeddings. Using pre-trained GloVe embeddings we can cast our vocabulary into real-numbered vectors of a dimension of our choosing. These vector representations possess the similarity expressed by the original tokens. We can determine how similar two vectors are by computing their cosine similarity or euclidean distance. We can illustrate the power of GloVe embeddings using the tokens from the previous example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ".vector_cache\\glove.6B.zip: 862MB [08:31, 1.69MB/s]                           \n",
      " 99%|█████████▉| 396587/400000 [00:11<00:00, 34150.93it/s]tensor([ 0.3765,  1.2426, -0.3974, -0.5318,  1.1870,  1.5091, -0.8417,  0.6788,\n",
      "        -0.2581, -0.4798,  0.1782,  0.7467, -0.1347, -0.9236,  0.9562,  0.2057,\n",
      "        -1.2239, -0.0550,  0.5618,  0.7808, -0.0441,  1.5692, -0.0668,  0.2514,\n",
      "         1.0403, -2.1412, -0.3199, -0.7717, -0.0292,  0.0471,  1.4145, -0.2327,\n",
      "        -0.3443,  0.2270,  0.8857, -0.2018, -0.1517,  0.3621,  0.6495, -0.6872,\n",
      "        -0.0682,  0.5360, -0.1529, -0.9016,  0.3896, -0.5230, -0.3219, -2.4262,\n",
      "         0.3005,  0.3389])\n"
     ]
    }
   ],
   "source": [
    "# loading GloVe vectors\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Example of a GloVe Embedding:\n tensor([ 0.3765,  1.2426, -0.3974, -0.5318,  1.1870,  1.5091, -0.8417,  0.6788,\n        -0.2581, -0.4798,  0.1782,  0.7467, -0.1347, -0.9236,  0.9562,  0.2057,\n        -1.2239, -0.0550,  0.5618,  0.7808, -0.0441,  1.5692, -0.0668,  0.2514,\n         1.0403, -2.1412, -0.3199, -0.7717, -0.0292,  0.0471,  1.4145, -0.2327,\n        -0.3443,  0.2270,  0.8857, -0.2018, -0.1517,  0.3621,  0.6495, -0.6872,\n        -0.0682,  0.5360, -0.1529, -0.9016,  0.3896, -0.5230, -0.3219, -2.4262,\n         0.3005,  0.3389])\nDifference Between Time and Room:\ntensor([0.6698])\n\nDifference Between Year and Old:\ntensor([0.5512])\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a GloVe Embedding:\\n\", glove[\"daughter\"])\n",
    "\n",
    "print(\"Difference Between Time and Room:\\n{}\".format(torch.cosine_similarity(glove[\"time\"].unsqueeze(0), glove[\"room\"].unsqueeze(0))))\n",
    "print(\"\\nDifference Between Year and Old:\\n{}\".format(torch.cosine_similarity(glove[\"year\"].unsqueeze(0), glove[\"old\"].unsqueeze(0))))"
   ]
  },
  {
   "source": [
    "From the above code-snippet we can see an example of a GloVe embedding for the word \"daughter\". We can also see that the GloVe embeddings are able to capture the fact that \"year\" and \"old\" are more similar than \"time\" and \"room\". \n",
    "\n",
    "## Word2Vec\n",
    "In certain contexts it may be worth it to train your own word embeddings. GloVe vectors do not take into account the context in which your words are used the most. You can use Python's Word2Vec library to train a word embedding on a desired vocabulary. Consider the following:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(sentences=[words], window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec Vocab:\n\n{'Even': <gensim.models.keyedvectors.Vocab object at 0x0000025923353988>, 'Mommy': <gensim.models.keyedvectors.Vocab object at 0x0000025923353FC8>, 'ha': <gensim.models.keyedvectors.Vocab object at 0x0000025923353E08>, 'fun': <gensim.models.keyedvectors.Vocab object at 0x00000259235BAC48>, 'with': <gensim.models.keyedvectors.Vocab object at 0x000002592339AB48>, 'this': <gensim.models.keyedvectors.Vocab object at 0x000002592339AB08>, 'one': <gensim.models.keyedvectors.Vocab object at 0x000002592339A908>, '!': <gensim.models.keyedvectors.Vocab object at 0x000002592339A448>, ':': <gensim.models.keyedvectors.Vocab object at 0x000002592339A3C8>, 'My': <gensim.models.keyedvectors.Vocab object at 0x000002592339A4C8>, 'four': <gensim.models.keyedvectors.Vocab object at 0x000002592339A288>, 'year': <gensim.models.keyedvectors.Vocab object at 0x000002592339A588>, 'old': <gensim.models.keyedvectors.Vocab object at 0x000002592339A488>, 'daughter': <gensim.models.keyedvectors.Vocab object at 0x000002592339A608>, 'love': <gensim.models.keyedvectors.Vocab object at 0x000002592339A5C8>, 'everything': <gensim.models.keyedvectors.Vocab object at 0x000002592339A688>, 'Barbie': <gensim.models.keyedvectors.Vocab object at 0x000002592339A648>, 'and': <gensim.models.keyedvectors.Vocab object at 0x000002592339A708>, 'the': <gensim.models.keyedvectors.Vocab object at 0x000002592339A788>, 'Rapunzel': <gensim.models.keyedvectors.Vocab object at 0x000002592339A108>, 'movie.': <gensim.models.keyedvectors.Vocab object at 0x000002592339A7C8>, 'This': <gensim.models.keyedvectors.Vocab object at 0x000002592339AD08>, 'game': <gensim.models.keyedvectors.Vocab object at 0x000002592339A948>, 'is': <gensim.models.keyedvectors.Vocab object at 0x000002592339AD48>, 'ton': <gensim.models.keyedvectors.Vocab object at 0x000002592339AC88>, 'of': <gensim.models.keyedvectors.Vocab object at 0x000002592339AE88>, ',': <gensim.models.keyedvectors.Vocab object at 0x000002592339ACC8>, 'even': <gensim.models.keyedvectors.Vocab object at 0x000002592339AEC8>, 'for': <gensim.models.keyedvectors.Vocab object at 0x000002592339AE08>, 'a': <gensim.models.keyedvectors.Vocab object at 0x000002592339A508>, '42': <gensim.models.keyedvectors.Vocab object at 0x000002592339AE48>, 'old.': <gensim.models.keyedvectors.Vocab object at 0x000002592339A8C8>, 'We': <gensim.models.keyedvectors.Vocab object at 0x000002592339AF88>, 'playing': <gensim.models.keyedvectors.Vocab object at 0x000002592339A9C8>, 'it': <gensim.models.keyedvectors.Vocab object at 0x000002592339AFC8>, 'together.': <gensim.models.keyedvectors.Vocab object at 0x000002592339AAC8>, 'decorating': <gensim.models.keyedvectors.Vocab object at 0x000002592339A2C8>, 'all': <gensim.models.keyedvectors.Vocab object at 0x000002592339A988>, 'room': <gensim.models.keyedvectors.Vocab object at 0x000002592339AA88>, 'finding': <gensim.models.keyedvectors.Vocab object at 0x000002592339ABC8>, 'gems.': <gensim.models.keyedvectors.Vocab object at 0x000002592339A148>, 'What': <gensim.models.keyedvectors.Vocab object at 0x0000025923327908>, 'better': <gensim.models.keyedvectors.Vocab object at 0x00000259233278C8>, 'she': <gensim.models.keyedvectors.Vocab object at 0x0000025923327B48>, 'can': <gensim.models.keyedvectors.Vocab object at 0x00000259233277C8>, 'play': <gensim.models.keyedvectors.Vocab object at 0x0000025923327508>, 'alone': <gensim.models.keyedvectors.Vocab object at 0x0000025923327EC8>, 'I': <gensim.models.keyedvectors.Vocab object at 0x0000025923327D88>, 'get': <gensim.models.keyedvectors.Vocab object at 0x0000025923327F48>, 'some': <gensim.models.keyedvectors.Vocab object at 0x00000259233272C8>, 'me': <gensim.models.keyedvectors.Vocab object at 0x0000025923327448>, 'time': <gensim.models.keyedvectors.Vocab object at 0x0000025923327CC8>}\n"
     ]
    }
   ],
   "source": [
    "word2vec_vocab = word2vec.wv.vocab\n",
    "print(\"Word2Vec Vocab:\\n\\n{}\".format(word2vec_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec Representation of 'daughter':\n\n[-1.2444714e-03  1.4156338e-03 -1.4909385e-03 -1.7089532e-03\n  3.2886292e-03 -8.2796864e-04 -2.0006793e-03  1.7300280e-03\n -3.9787362e-03 -3.7500761e-03 -4.5552701e-03 -4.2422013e-03\n  2.8239305e-05 -7.8168540e-04 -2.2519373e-03  2.2041863e-03\n  4.6283044e-03  4.8661926e-03  1.6543671e-03 -9.9729467e-04\n  2.2404236e-03  3.9539691e-03 -1.3324495e-03  2.2458609e-03\n -4.5952527e-03 -1.6946433e-03 -3.3995342e-03 -4.3935794e-03\n -2.9841559e-03  2.8422219e-03  1.3971644e-03  4.8590996e-03\n -4.5874314e-03 -4.5272587e-03 -2.0506987e-03  1.4308434e-03\n -1.1574064e-03 -4.6182331e-03 -4.4134702e-03  1.9977742e-03\n  8.7627425e-04 -3.8978001e-03  4.0437295e-03 -3.4957377e-03\n -3.5399715e-03  4.7269608e-03 -3.5634337e-03  4.9276939e-03\n  3.2719807e-03  9.2952105e-04  3.2853750e-03  1.4551891e-03\n -3.1109690e-04  2.1551419e-03  2.0533291e-04 -3.0298058e-03\n  3.4850249e-03 -4.1296319e-03 -3.8312501e-03  3.1538843e-03\n -1.5336482e-03  4.5721438e-03 -2.8174086e-03  9.5928990e-04\n -1.1557315e-03 -4.4133058e-03  4.6267938e-03  4.8469128e-03\n -1.2360604e-03  4.7034855e-04 -2.0122619e-03  4.7635157e-03\n  6.9642992e-04  2.0310380e-03 -3.4914410e-03 -8.8107656e-04\n  3.4070150e-03 -4.1128970e-03  2.7081810e-03 -1.9053846e-03\n -1.5954368e-03 -3.1606546e-03  4.5562971e-05 -1.6904329e-03\n  2.2948445e-04 -1.2022037e-03  3.3042913e-03  4.8953155e-03\n -1.0597373e-04  3.3351365e-03  2.8294504e-03 -2.5304190e-03\n  3.1767730e-03 -4.4637895e-03 -1.8535041e-03  2.2630487e-03\n  3.0196835e-03  4.8317006e-03 -1.8165465e-03  1.8564783e-03]\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2Vec Representation of 'daughter':\\n\\n{}\".format(word2vec.wv[\"daughter\"]))"
   ]
  },
  {
   "source": [
    "The above snippet provides the Word2Vec model with a vocabulary of one sentence. Then, it computes the Word2Vec representation of \"daughter\". Just like GloVe vectors, Word2Vec embeddings retain the similarity of similar tokens. \n",
    "\n",
    "A major drawback of Word2Vec is that it requires a large corpus to be trained on. If a small corpus is provided then the similarity between the words in the corpus will not be captured accurately. This will ultimately produce a poor model that cannot learn from its inputs. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}