{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as rqst\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting news dara using RSS feeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NY Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_feed = feedparser.parse('https://rss.nytimes.com/services/xml/rss/nyt/Arts.xml')#feedparsing rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'A Golden Team, a Terrible Title and a Show That Vanished', 'title_detail': {'type': 'text/plain', 'language': None, 'base': 'https://rss.nytimes.com/services/xml/rss/nyt/Arts.xml', 'value': 'A Golden Team, a Terrible Title and a Show That Vanished'}, 'links': [{'rel': 'alternate', 'type': 'text/html', 'href': 'https://www.nytimes.com/2020/11/02/theater/a-pray-by-blecht-sondheim-bernstein.html'}, {'href': 'https://www.nytimes.com/2020/11/02/theater/a-pray-by-blecht-sondheim-bernstein.html', 'rel': 'standout', 'type': 'text/html'}], 'link': 'https://www.nytimes.com/2020/11/02/theater/a-pray-by-blecht-sondheim-bernstein.html', 'id': 'https://www.nytimes.com/2020/11/02/theater/a-pray-by-blecht-sondheim-bernstein.html', 'guidislink': False, 'summary': 'Would you like to see a new musical from the people who brought you “West Side Story”? For better or worse, you probably never will.', 'summary_detail': {'type': 'text/html', 'language': None, 'base': 'https://rss.nytimes.com/services/xml/rss/nyt/Arts.xml', 'value': 'Would you like to see a new musical from the people who brought you “West Side Story”? For better or worse, you probably never will.'}, 'authors': [{'name': 'Jesse Green'}], 'author': 'Jesse Green', 'author_detail': {'name': 'Jesse Green'}, 'published': 'Mon, 02 Nov 2020 20:08:17 +0000', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=11, tm_mday=2, tm_hour=20, tm_min=8, tm_sec=17, tm_wday=0, tm_yday=307, tm_isdst=0), 'tags': [{'term': 'Theater', 'scheme': 'http://www.nytimes.com/namespaces/keywords/des', 'label': None}, {'term': 'A Pray by Blecht (Play)', 'scheme': 'http://www.nytimes.com/namespaces/keywords/nyt_ttl', 'label': None}, {'term': 'Lincoln Center Theater', 'scheme': 'http://www.nytimes.com/namespaces/keywords/nyt_org', 'label': None}, {'term': 'Brecht, Bertolt', 'scheme': 'http://www.nytimes.com/namespaces/keywords/nyt_per', 'label': None}, {'term': 'Guare, John', 'scheme': 'http://www.nytimes.com/namespaces/keywords/nyt_per', 'label': None}, {'term': 'Robbins, Jerome', 'scheme': 'http://www.nytimes.com/namespaces/keywords/nyt_per', 'label': None}, {'term': 'Sondheim, Stephen', 'scheme': 'http://www.nytimes.com/namespaces/keywords/nyt_per', 'label': None}, {'term': 'Bernstein, Leonard', 'scheme': 'http://www.nytimes.com/namespaces/keywords/nyt_per', 'label': None}, {'term': 'Mostel, Josh', 'scheme': 'http://www.nytimes.com/namespaces/keywords/nyt_per', 'label': None}], 'media_content': [{'height': '151', 'medium': 'image', 'url': 'https://static01.nyt.com/images/2020/11/03/arts/02unopened-blecht-1/02unopened-blecht-1-moth.jpg', 'width': '151'}], 'media_credit': [{'content': 'Alfred Eisenstaedt/The LIFE Picture Collection, via Getty Images'}], 'credit': 'Alfred Eisenstaedt/The LIFE Picture Collection, via Getty Images', 'content': [{'type': 'text/html', 'language': None, 'base': 'https://rss.nytimes.com/services/xml/rss/nyt/Arts.xml', 'value': 'From left, Stephen Sondheim, Leonard Bernstein and Jerome Robbins, here working on &ldquo;West Side Story,&rdquo; reconnected on a political musical that died before it opened. Twice.'}]}\n"
     ]
    }
   ],
   "source": [
    "print(ny_feed['entries'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting metadata\n",
    "titles = []\n",
    "dates = []\n",
    "links = []\n",
    "authors = []\n",
    "texts = []\n",
    "for article in ny_feed['entries']:\n",
    "    titles.append(article['title'])\n",
    "    dates.append(article['published'])\n",
    "    links.append(article['link'])\n",
    "    html = rqst.urlopen(article['link'])\n",
    "    bs = BeautifulSoup(html, features='html.parser')\n",
    "    targets = bs.select('.css-158dogj')\n",
    "    text = ''\n",
    "    for target in targets:\n",
    "        text += target.text\n",
    "        text += ' '\n",
    "    texts.append(text)\n",
    "    if 'author' in article:\n",
    "        authors.append(article['author'])\n",
    "    else:\n",
    "        authors.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "ny_data={\"title\": titles, \"date\": dates,\"link\": links, \"author\": authors, \"text\": texts}\n",
    "ny=pd.DataFrame(ny_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Golden Team, a Terrible Title and a Show Tha...</td>\n",
       "      <td>Mon, 02 Nov 2020 20:08:17 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/theater/a-p...</td>\n",
       "      <td>Jesse Green</td>\n",
       "      <td>How do you top “West Side Story”? If you’re Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnny Depp Loses Court Case Against Newspaper...</td>\n",
       "      <td>Mon, 02 Nov 2020 17:01:15 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/arts/johnny...</td>\n",
       "      <td>Alex Marshall</td>\n",
       "      <td>LONDON — Johnny Depp on Monday lost his court ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With New Show, Tschabalala Self Explores Black...</td>\n",
       "      <td>Mon, 02 Nov 2020 18:31:41 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/arts/design...</td>\n",
       "      <td>Robin Pogrebin</td>\n",
       "      <td>NEW HAVEN — It was warm for October, the sun f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dancing on Grass and Concrete at New York City...</td>\n",
       "      <td>Mon, 02 Nov 2020 18:38:35 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/arts/dance/...</td>\n",
       "      <td>Gia Kourlas</td>\n",
       "      <td>When it comes to digital site-specific work, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>City Pages, the Alt-Weekly Where Music Writing...</td>\n",
       "      <td>Mon, 02 Nov 2020 18:29:13 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/arts/music/...</td>\n",
       "      <td>Keith Harris</td>\n",
       "      <td>When a young music journalist moved to New Yor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  A Golden Team, a Terrible Title and a Show Tha...   \n",
       "1  Johnny Depp Loses Court Case Against Newspaper...   \n",
       "2  With New Show, Tschabalala Self Explores Black...   \n",
       "3  Dancing on Grass and Concrete at New York City...   \n",
       "4  City Pages, the Alt-Weekly Where Music Writing...   \n",
       "\n",
       "                              date  \\\n",
       "0  Mon, 02 Nov 2020 20:08:17 +0000   \n",
       "1  Mon, 02 Nov 2020 17:01:15 +0000   \n",
       "2  Mon, 02 Nov 2020 18:31:41 +0000   \n",
       "3  Mon, 02 Nov 2020 18:38:35 +0000   \n",
       "4  Mon, 02 Nov 2020 18:29:13 +0000   \n",
       "\n",
       "                                                link          author  \\\n",
       "0  https://www.nytimes.com/2020/11/02/theater/a-p...     Jesse Green   \n",
       "1  https://www.nytimes.com/2020/11/02/arts/johnny...   Alex Marshall   \n",
       "2  https://www.nytimes.com/2020/11/02/arts/design...  Robin Pogrebin   \n",
       "3  https://www.nytimes.com/2020/11/02/arts/dance/...     Gia Kourlas   \n",
       "4  https://www.nytimes.com/2020/11/02/arts/music/...    Keith Harris   \n",
       "\n",
       "                                                text  \n",
       "0  How do you top “West Side Story”? If you’re Le...  \n",
       "1  LONDON — Johnny Depp on Monday lost his court ...  \n",
       "2  NEW HAVEN — It was warm for October, the sun f...  \n",
       "3  When it comes to digital site-specific work, t...  \n",
       "4  When a young music journalist moved to New Yor...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ny.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny.to_csv('./ny.csv', index=True)#export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_feed = feedparser.parse('http://rss.cnn.com/services/podcasting/cnn10/rss.xml')#feedparsing rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'CNN10 - 10/29/20', 'title_detail': {'type': 'text/plain', 'language': 'en-US', 'base': 'http://rss.cnn.com/services/podcasting/cnn10/rss.xml', 'value': 'CNN10 - 10/29/20'}, 'links': [{'rel': 'alternate', 'type': 'text/html', 'href': 'http://rss.cnn.com/~r/services/podcasting/cnn10/rss/~3/sMOCLQQW_94/ten-1029.cnn_3460289_ios_1240.mp4'}, {'length': '', 'type': 'video/mp4', 'href': 'http://pmd.cdn.turner.com/cnn/big/cnn10/2020/10/28/ten-1029.cnn_3460289_ios_1240.mp4', 'rel': 'enclosure'}], 'link': 'http://rss.cnn.com/~r/services/podcasting/cnn10/rss/~3/sMOCLQQW_94/ten-1029.cnn_3460289_ios_1240.mp4', 'summary': 'Today\\'s show covers storms in Vietnam and the U.S., international travel in the era of coronavirus, and a newly discovered reef that\\'s the size of a skyscraper.<img alt=\"\" height=\"1\" src=\"http://feeds.feedburner.com/~r/services/podcasting/cnn10/rss/~4/sMOCLQQW_94\" width=\"1\" />', 'summary_detail': {'type': 'text/html', 'language': 'en-US', 'base': 'http://rss.cnn.com/services/podcasting/cnn10/rss.xml', 'value': 'Today\\'s show covers storms in Vietnam and the U.S., international travel in the era of coronavirus, and a newly discovered reef that\\'s the size of a skyscraper.<img alt=\"\" height=\"1\" src=\"http://feeds.feedburner.com/~r/services/podcasting/cnn10/rss/~4/sMOCLQQW_94\" width=\"1\" />'}, 'authors': [{'name': 'CNN'}, {'name': 'CNN'}], 'author': 'CNN', 'author_detail': {'name': 'CNN'}, 'tags': [{'term': 'Education', 'scheme': None, 'label': None}], 'published': 'Wed, 28 Oct 2020 19:10:31 EDT', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=10, tm_mday=28, tm_hour=23, tm_min=10, tm_sec=31, tm_wday=2, tm_yday=302, tm_isdst=0), 'id': 'http://pmd.cdn.turner.com/cnn/big/cnn10/2020/10/28/ten-1029.cnn_3460289_ios_1240.mp4', 'guidislink': False, 'content': [{'type': 'text/plain', 'language': 'en-US', 'base': 'http://rss.cnn.com/services/podcasting/cnn10/rss.xml', 'value': \"Today's show covers storms in Vietnam and the U.S., international travel in the era of coronavirus, and a newly discovered reef that's the size of a skyscraper.\"}], 'itunes_duration': '10:00', 'source': {'href': '', 'title': 'CNN10 - 10/29/20'}, 'feedburner_origlink': 'http://pmd.cdn.turner.com/cnn/big/cnn10/2020/10/28/ten-1029.cnn_3460289_ios_1240.mp4'}\n"
     ]
    }
   ],
   "source": [
    "print(cnn_feed['entries'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-64b3dd22d097>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mlinks2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrqst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'link'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.css-158dogj'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m          \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontains_replacement_characters\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m              self.builder.prepare_markup(\n\u001b[1;32m--> 278\u001b[1;33m                  markup, from_encoding, exclude_encodings=exclude_encodings)):\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mprepare_markup\u001b[1;34m(self, markup, user_specified_encoding, document_declared_encoding, exclude_encodings)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0mtry_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0muser_specified_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         dammit = UnicodeDammit(markup, try_encodings, is_html=True,\n\u001b[1;32m--> 237\u001b[1;33m                                exclude_encodings=exclude_encodings)\n\u001b[0m\u001b[0;32m    238\u001b[0m         yield (dammit.markup, dammit.original_encoding,\n\u001b[0;32m    239\u001b[0m                \u001b[0mdammit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeclared_html_encoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, override_encodings, smart_quotes_to, is_html, exclude_encodings)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36mencodings\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;31m# encoding.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_usable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\dammit.py\u001b[0m in \u001b[0;36mchardet_dammit\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;31m#import chardet.constants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m#chardet.constants._debug = 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chardet\\__init__.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(byte_str)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chardet\\universaldetector.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[0;32m    213\u001b[0m                                    \u001b[1;34m'confidence'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chardet\\charsetgroupprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\chardet\\sbcharsetprober.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, byte_str)\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;31m# XXX: Order is in range 1-64, so one would think we want 0-63 here,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;31m#      but that leads to 27 more test failures than before.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchar_to_order_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;31m# XXX: This was SYMBOL_CAT_ORDER before, with a value of 250, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[1;31m#      CharacterCategory.SYMBOL is actually 253, so we use CONTROL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#getting metadata\n",
    "titles2 = []\n",
    "dates2 = []\n",
    "links2 = []\n",
    "authors2 = []\n",
    "texts2 = []\n",
    "for article in cnn_feed['entries']:\n",
    "    titles2.append(article['title'])\n",
    "    dates2.append(article['published'])\n",
    "    links2.append(article['link'])\n",
    "    html = rqst.urlopen(article['link'])\n",
    "    bs = BeautifulSoup(html, features='html.parser')\n",
    "    targets = bs.select('.css-158dogj')\n",
    "    text = ''\n",
    "    for target in targets:\n",
    "        text += target.text\n",
    "        text += ' '\n",
    "    texts2.append(text)\n",
    "    if 'author' in article:\n",
    "        authors2.append(article['author'])\n",
    "    else:\n",
    "        authors2.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "cnn_data={\"title\": titles2, \"date\": dates2,\"link\": links2, \"author\": authors2, \"text\": texts2}\n",
    "cnn=pd.DataFrame(cnn_data)\n",
    "cnn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.to_csv('./cnn.csv', index=True)#export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc_feed = feedparser.parse('https://rss.cbc.ca/lineup/topstories.xml')#feedparsing rss\n",
    "print(cbc_feed['entries'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting metadata\n",
    "titles3 = []\n",
    "dates3 = []\n",
    "links3 = []\n",
    "authors3 = []\n",
    "texts3 = []\n",
    "for article in cbc_feed['entries']:\n",
    "    titles3.append(article['title'])\n",
    "    dates3.append(article['published'])\n",
    "    links3.append(article['link'])\n",
    "    html = rqst.urlopen(article['link'])\n",
    "    bs = BeautifulSoup(html, features='html.parser')\n",
    "    targets = bs.find(\"body\").select('.story')[0].select('p')\n",
    "    text = ''\n",
    "    for target in targets:\n",
    "        text += target.text\n",
    "        text += ' '\n",
    "    texts3.append(text)\n",
    "    if 'author' in article:\n",
    "        authors3.append(article['author'])\n",
    "    else:\n",
    "        authors3.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "cbc_data={\"title\": titles3, \"date\": dates3,\"link\": links3, \"author\": authors3, \"text\": texts3}\n",
    "cbc=pd.DataFrame(cbc_data)\n",
    "cbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc.to_csv('./cbc.csv', index=True)#export to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc_feed = feedparser.parse('http://feeds.bbci.co.uk/news/rss.xml')#feedparsing rss\n",
    "print(bbc_feed['entries'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting metadata\n",
    "titles4 = []\n",
    "dates4 = []\n",
    "links4 = []\n",
    "authors4 = []\n",
    "texts4 = []\n",
    "for article in bbc_feed['entries']:\n",
    "    titles4.append(article['title'])\n",
    "    dates4.append(article['published'])\n",
    "    links4.append(article['link'])\n",
    "    html = rqst.urlopen(article['link'])\n",
    "    bs = BeautifulSoup(html, features='html.parser')\n",
    "    targets = bs.select('.css-83cqas-RichTextContainer')\n",
    "    text = ''\n",
    "    for target in targets:\n",
    "        text += target.text\n",
    "        text += ' '\n",
    "    texts4.append(text)\n",
    "    if 'author' in article:\n",
    "        authors4.append(article['author'])\n",
    "    else:\n",
    "        authors4.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe\n",
    "bbc_data={\"title\": titles4, \"date\": dates4,\"link\": links4, \"author\": authors4, \"text\": texts4}\n",
    "bbc=pd.DataFrame(bbc_data)\n",
    "bbc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc.to_csv('./bbc.csv', index=True)#export to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.bbc.co.uk/news/uk-54773196'\n",
    "html = rqst.urlopen(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(html, features='html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.select('.css-83cqas-RichTextContainer')[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Text Processing for NY times dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Golden Team, a Terrible Title and a Show Tha...</td>\n",
       "      <td>Mon, 02 Nov 2020 20:08:17 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/theater/a-p...</td>\n",
       "      <td>Jesse Green</td>\n",
       "      <td>How do you top “West Side Story”? If you’re Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Johnny Depp Loses Court Case Against Newspaper...</td>\n",
       "      <td>Mon, 02 Nov 2020 17:01:15 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/arts/johnny...</td>\n",
       "      <td>Alex Marshall</td>\n",
       "      <td>LONDON — Johnny Depp on Monday lost his court ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>With New Show, Tschabalala Self Explores Black...</td>\n",
       "      <td>Mon, 02 Nov 2020 18:31:41 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/arts/design...</td>\n",
       "      <td>Robin Pogrebin</td>\n",
       "      <td>NEW HAVEN — It was warm for October, the sun f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Dancing on Grass and Concrete at New York City...</td>\n",
       "      <td>Mon, 02 Nov 2020 18:38:35 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/arts/dance/...</td>\n",
       "      <td>Gia Kourlas</td>\n",
       "      <td>When it comes to digital site-specific work, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>City Pages, the Alt-Weekly Where Music Writing...</td>\n",
       "      <td>Mon, 02 Nov 2020 18:29:13 +0000</td>\n",
       "      <td>https://www.nytimes.com/2020/11/02/arts/music/...</td>\n",
       "      <td>Keith Harris</td>\n",
       "      <td>When a young music journalist moved to New Yor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  A Golden Team, a Terrible Title and a Show Tha...   \n",
       "1           1  Johnny Depp Loses Court Case Against Newspaper...   \n",
       "2           2  With New Show, Tschabalala Self Explores Black...   \n",
       "3           3  Dancing on Grass and Concrete at New York City...   \n",
       "4           4  City Pages, the Alt-Weekly Where Music Writing...   \n",
       "\n",
       "                              date  \\\n",
       "0  Mon, 02 Nov 2020 20:08:17 +0000   \n",
       "1  Mon, 02 Nov 2020 17:01:15 +0000   \n",
       "2  Mon, 02 Nov 2020 18:31:41 +0000   \n",
       "3  Mon, 02 Nov 2020 18:38:35 +0000   \n",
       "4  Mon, 02 Nov 2020 18:29:13 +0000   \n",
       "\n",
       "                                                link          author  \\\n",
       "0  https://www.nytimes.com/2020/11/02/theater/a-p...     Jesse Green   \n",
       "1  https://www.nytimes.com/2020/11/02/arts/johnny...   Alex Marshall   \n",
       "2  https://www.nytimes.com/2020/11/02/arts/design...  Robin Pogrebin   \n",
       "3  https://www.nytimes.com/2020/11/02/arts/dance/...     Gia Kourlas   \n",
       "4  https://www.nytimes.com/2020/11/02/arts/music/...    Keith Harris   \n",
       "\n",
       "                                                text  \n",
       "0  How do you top “West Side Story”? If you’re Le...  \n",
       "1  LONDON — Johnny Depp on Monday lost his court ...  \n",
       "2  NEW HAVEN — It was warm for October, the sun f...  \n",
       "3  When it comes to digital site-specific work, t...  \n",
       "4  When a young music journalist moved to New Yor...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ny.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     0\n",
       "date      0\n",
       "link      0\n",
       "author    2\n",
       "text      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleaning\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of missing values is low, so we can drop the rows with nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine title and text data\n",
    "df['title_text'] = df['title'] + df['text']\n",
    "\n",
    "#Separate data into training and testing, maybe do this step as a maybe when there are more data points\n",
    "df_train,df_test = train_test_split(df,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function for text processing using TF and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#def text_processing(text, is_tfidf):\n",
    "#    vectorizer = TfidfVectorizer(stop_words = 'english', \n",
    "#                                 use_idf = is_tfidf, \n",
    "#                                 max_features = 200, \n",
    "#                                 ngram_range = (2,3)).fit(text)\n",
    "#    return vectorizer.transform(text), vectorizer.get_feature_names()\n",
    "\n",
    "def text_processing(text, is_tfidf):\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english', \n",
    "                                 use_idf = is_tfidf, \n",
    "                                 max_features = 200, \n",
    "                                 ngram_range = (2,3)).fit(text)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Combine title and text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the title of the article is related to the content of the article. So, in this context, combining the text and title before extracting features seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Representation based on TF (Term Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency depends on the frequency of a word in the current article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document representation based on TF\n",
    "vectorizer = text_processing(df_train['title_text'], False)\n",
    "train_text_rep = vectorizer.transform(df_train['title_text'])\n",
    "test_text_rep = vectorizer.transform(df_test['title_text'])\n",
    "\n",
    "X_train_tf = pd.concat([df_train.drop(['title', 'text', 'title_text'], axis=1),\n",
    "           pd.DataFrame(data = train_text_rep.toarray(), columns = vectorizer.get_feature_names())], axis = 1)\n",
    "\n",
    "X_test_tf = pd.concat([df_test.drop(['title', 'text', 'title_text'], axis=1),\n",
    "           pd.DataFrame(data = test_text_rep.toarray(), columns = vectorizer.get_feature_names())], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Representation based on TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TF is problematic because words that are frequent but not necessarily useful (e.g. the) will have a high score. TF-IDF, combines TF and IDF, which measures the how rare a word is accross articles to solve the limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document representation based on TF-IDF\n",
    "vectorizer = text_processing(df_train['title_text'], True)\n",
    "train_text_rep = vectorizer.transform(df_train['title_text'])\n",
    "test_text_rep = vectorizer.transform(df_test['title_text'])\n",
    "\n",
    "X_train_tfidf = pd.concat([df_train.drop(['title', 'text', 'title_text'], axis=1),\n",
    "           pd.DataFrame(data = train_text_rep.toarray(), columns = vectorizer.get_feature_names())], axis = 1)\n",
    "\n",
    "X_test_tfidf = pd.concat([df_test.drop(['title', 'text', 'title_text'], axis=1),\n",
    "           pd.DataFrame(data = test_text_rep.toarray(), columns = vectorizer.get_feature_names())], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2:  Separate title and text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if you have text features that are not related, this option would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Representation based on TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On Training\n",
    "\n",
    "title_vectorizer = text_processing(df_train['title'], False)\n",
    "title_rep = title_vectorizer.transform(df_train['title'])\n",
    "text_vectorizer = text_processing(df_train['text'], False)\n",
    "text_rep = text_vectorizer.transform(df_train['text'])\n",
    "\n",
    "X_train_tf = pd.concat([df_train.drop(['title', 'text'], axis=1),\n",
    "           pd.DataFrame(data = title_rep.toarray(), columns = title_vectorizer.get_feature_names()),\n",
    "           pd.DataFrame(data = text_rep.toarray(), columns = text_vectorizer.get_feature_names())], axis = 1)\n",
    "\n",
    "#On test data\n",
    "\n",
    "title_rep = title_vectorizer.transform(df_test['title'])\n",
    "text_rep = text_vectorizer.transform(df_test['text'])\n",
    "\n",
    "X_test_tf = pd.concat([df_test.drop(['title', 'text'], axis=1),\n",
    "           pd.DataFrame(data = title_rep.toarray(), columns = title_vectorizer.get_feature_names()),\n",
    "           pd.DataFrame(data = text_rep.toarray(), columns = text_vectorizer.get_feature_names())], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Representation based on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Training\n",
    "\n",
    "title_vectorizer = text_processing(df_train['title'], True)\n",
    "title_rep = title_vectorizer.transform(df_train['title'])\n",
    "text_vectorizer = text_processing(df_train['text'], True)\n",
    "text_rep = text_vectorizer.transform(df_train['text'])\n",
    "\n",
    "X_train_tfidf = pd.concat([df_train.drop(['title', 'text'], axis=1),\n",
    "           pd.DataFrame(data = title_rep.toarray(), columns = title_vectorizer.get_feature_names()),\n",
    "           pd.DataFrame(data = text_rep.toarray(), columns = text_vectorizer.get_feature_names())], axis = 1)\n",
    "\n",
    "#On test data\n",
    "\n",
    "title_rep = title_vectorizer.transform(df_test['title'])\n",
    "text_rep = text_vectorizer.transform(df_test['text'])\n",
    "\n",
    "X_test_tfidf = pd.concat([df_test.drop(['title', 'text'], axis=1),\n",
    "           pd.DataFrame(data = title_rep.toarray(), columns = title_vectorizer.get_feature_names()),\n",
    "           pd.DataFrame(data = text_rep.toarray(), columns = text_vectorizer.get_feature_names())], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
