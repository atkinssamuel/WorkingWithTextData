{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit (conda)",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "cf5781ee9688be887a830af3862a813ef5b0a355a8789de10c6994358407057c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# SkLearn/NLTK Library Discussion\n",
    "## Requirements\n",
    "- Introduce scikit-learn, nltk, and other text processing libraries\n",
    "- Explain basic feature extraction from text data\n",
    "    - Number of words\n",
    "    - Number of characters\n",
    "    - Average word length\n",
    "    - Number of stopwords\n",
    "    - Other feature extraction techniques that may be relevant\n",
    "- Explan advanced text processing\n",
    "    - N-grams\n",
    "    - Term Frequency (TF)\n",
    "    - Inverse Document Frequency (IDF)\n",
    "    - Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    - \"Bag of Words\" document representation\n",
    "    - Word embedding\n",
    "    - Other text processing methods that may be important"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Document Feature Extraction\n",
    "## TF/IDF\n",
    "Term Frequency (TF) and Inverse Document Frequency (IDF) are useful data that will inform us as to which terms are the most relevant to a given document in a corpus. TF simply measures the frequency of a given word in a document. This measurement is useful because typically, common words are relevant to the meaning of that document. \n",
    "\n",
    "Suppose we have access to all of the articles on Wikipedia. Our corpus, in this context, is all of these Wikipedia articles combined. Each of these articles would be its own entity called a document. Document Frequency (DF) is a measure of how frequently a given word appears in the entire corpus of text. The IDF is the inverst of the DF:\n",
    "\n",
    "$IDF = \\frac{1}{DF}$\n",
    "\n",
    "Using the TF and the IDF we can measure how important and unique a given word is for a particular document. Since word frequencies are distributed exponentially, we use the log of the IDF to obtain a measure for the value of a word in a given document:\n",
    "\n",
    "Word Value (WV) $= TF \\cdot log(IDF)$\n",
    "\n",
    "This approach treats the documents in a corpus as a \"bag of words\". This means that this approach does not take into account the meaning of sentences or the contexts in which these words are used in. This restricts the applications of this method severely. \n",
    "\n",
    "### TF/IDF Application\n",
    "Suppose we wish to search our Wikipedia corpus for a document most relevant to Serena Williams. We could compute the WV for \"Serena Williams\" accross all of the documents. Then, we could the set of documents that had the highest \"Serena Williams\" WVs. This would be a reasonably effective way to query a corpus for relevant documents. \n",
    "\n",
    "# N-Gram Models\n",
    "N-Gram models are very useful models in the field of text analysis. An N-Gram model can predict the probability of a word occurring based on the occurence of its N-1 words. These models have a tremendous amount of use cases. They can be used to determine which word belongs in a particular sentence for the purpose of text generation. They can be used to detect spelling errors in sentences. They can also be used as speech recognition engines. \n",
    "\n",
    "We will illustrate a simple example of how an N-Gram model can be used to detect a spelling error. Suppose we are given the following set of sentences:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Today I went to the store. Yesterday it took me 10 minutes to drive there. Today, it took me 15 minutes to get there and 15 minuets to get back.\""
   ]
  },
  {
   "source": [
    "\"minutes\" was spelled as \"minuets\" in the third sentence. If we implement a simple bi-gram (2-gram) model on this corpus, we can compute the probability of each word appearing as a function of the word appearing immediately before that word. The following snippet tokenizes the above sentence and computes the bi-gram frequencies:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(it, took)             2\n(to, get)              2\n(minute, to)           2\n(took, me)             2\n(minuet, to)           1\n(to, the)              1\n(and, 15)              1\n(10, minute)           1\n(there, and)           1\n(the, store.)          1\n(back, .)              1\n(store., Yesterday)    1\n(,, it)                1\n(to, drive)            1\n(Yesterday, it)        1\n(went, to)             1\n(get, there)           1\n(me, 10)               1\n(Today, ,)             1\n(there., Today)        1\n(me, 15)               1\n(15, minute)           1\n(I, went)              1\n(15, minuet)           1\n(drive, there.)        1\n(get, back)            1\n(Today, I)             1\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "tree_bank_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "words = tree_bank_tokenizer.tokenize(sentence)\n",
    "words = [wnlemmatizer.lemmatize(word) for word in words]\n",
    "n_grams_series = pd.Series(nltk.ngrams(words, 2))\n",
    "print(n_grams_series.value_counts())"
   ]
  },
  {
   "source": [
    "From the snippet above, we can see that the word \"minute\" appears twice before the word \"to\". In the original corpus, the word \"to\" appears 3 times, not including the time when it appeared after \"minuets\". Therefore, there is a probability of 2/3 that \"minuets\" should be \"minutes\" and a probability of 1/3 that it should be \"went\".\n",
    "\n",
    "In this simple example a bi-gram model was able to detect a spelling mistake. By using a large corpus we can achieve very effective N-gram models. Further, increasing the model's \"N\" we can sometimes increase the effectiveness of the model. Increasing \"N\" can also, however, harm the effectiveness of the model by using irrelevant words to determine the context of a given word. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Text Processing for Complex Models\n",
    "## Why is Text Processing Important?\n",
    "Natural language processing is a branch of artificial intelligence in which unstructured text data is broken down into information that can be analyzed by models. Especially in the age of social media, humans commuincate online through unstructured text messages all the time. Whether it be through Twitter, private messaging platforms like Facebook Messenger, or through product reviewing interfaces like Yelp. The ability to transform these unstructured blobs of text into forms that are machines can understand opens the door to a wide variety of applications. \n",
    "\n",
    "These applications primarily fall under text classification and generative models. Text classification agents try to uncover the intent, logic, and/or meaning behind blobs of text. This model type can be implemented in all sorts of contexts. For example, as an AI that can automatically sort through text reviews to characterize the performance of a fast-food chain. \n",
    "\n",
    "Processed text can also be used as an input to generative models. These generative models learn how to commuincate by training on large blobs of processed text. These models can be used to generate speeches or converse with customers, for example.\n",
    "\n",
    "## Tokenization\n",
    "When trying to analyze a corpus, some characters, a chain of words, or a set of reviews, we are initially provided with some text. This text may contain spelling errors, they may or may not obey the grammatical rules of the English language, and they may contain punctuation. For our analysis, we will consider Amazon review data. The following code snippets load the Amazon review dataset and display a handful of example reviews:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# importing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import nltk"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading data\n",
    "# train_data_file = bz2.BZ2File(\"amazon-review-data/train.ft.txt.bz2\")\n",
    "# test_data_file = bz2.BZ2File(\"amazon-review-data/test.ft.txt.bz2\")\n",
    "# train_lines = train_data_file.readlines()\n",
    "# test_lines = test_data_file.readlines()\n",
    "# del train_data_file, test_data_file\n",
    "# train_lines = [x.decode('utf-8') for x in train_lines]\n",
    "# test_lines = [x.decode('utf-8') for x in test_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Lines Length = 3600000\nTest Lines Length = 400000\nLength of Total Dataset = 4000000\nPercentage of Training Data = 90.0%\nPercentage of Testing Data = 10.0%\n"
     ]
    }
   ],
   "source": [
    "# dataset properties\n",
    "train_lines_len = len(train_lines)\n",
    "test_lines_len = len(test_lines)\n",
    "total_length = train_lines_len + test_lines_len\n",
    "print(\"Train Lines Length =\", train_lines_len)\n",
    "print(\"Test Lines Length =\", test_lines_len)\n",
    "\n",
    "total_length = train_lines_len + test_lines_len\n",
    "\n",
    "print(\"Length of Total Dataset =\", total_length)\n",
    "print(\"Percentage of Training Data = {}%\".format(round(train_lines_len / total_length * 100, 3)))\n",
    "print(\"Percentage of Testing Data = {}%\".format(round(test_lines_len / total_length * 100, 3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Amazon Example Review at Index 99\nCaution!: These tracks are not the \"original\" versions but are re-recorded versions. So, whether the tracks are \"remastered\" or not is irrelevant.\n\nAmazon Example Review at Index 90\nNo instructions included - do not trust seller: Promised with this item are \"Complete Instructions\" and the additional pledge that \"Sweet Graces will email you with the Supply List and Instruction sheets on purchase - so you can be ready ahead of time!\" I received none of this - only a plastic figurine and bracelet. To boot, Amazon claims they can do nothing to help me contact the seller. All I got was a phone number for the manufacturer. Let's hope that yields some results. Meanwhile, I'm wishing I had listened to previous feedback about this unreliable seller :/\n\nAmazon Example Review at Index 15\nDon't try to fool us with fake reviews.: It's glaringly obvious that all of the glowing reviews have been written by the same person, perhaps the author herself. They all have the same misspellings and poor sentence structure that is featured in the book. Who made Veronica Haddon think she is an author?\n\n"
     ]
    }
   ],
   "source": [
    "# amazon review examples\n",
    "np.random.seed(20)\n",
    "random_indices = [np.random.randint(100) for x in range(3)]\n",
    "for index in random_indices:\n",
    "    print(\"Amazon Example Review at Index {}\".format(index))\n",
    "    print(train_lines[index][11:])"
   ]
  },
  {
   "source": [
    "As shown above, these text reviews contain a few sentences strung together to form a review. These sentences contain punctuation, capital letters, slang, and spelling mistakes. Our goal when processing these blobs of text is to take these unstructured sentences and transform them into a set of tokens that a model can understand. Currently, the model cannot understand the meaning of these sentences.\n",
    "\n",
    "There are several natural language processing libraries in Python that allow us to create tokens from these unstructured sentences. The way in which we break these sentences apart will determine the effectiveness of the model.\n",
    "\n",
    "To create tokens, we must first consider the way in which we understand the English language. A sentence is made up of words. These words are separated by spaces. We can tokenize a sentence by splitting up the sentence using white-space as a delimiter. Consider the following text entry:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "i liked this album more then i thought i would: I heard a song or two and thought same o same o,but when i listened to songs like \"blue angel\",\"lanna\" and 'mama\" the hair just rose off my neck.Roy is trully an amazing singer with a talent you don't find much now days.\n\n"
     ]
    }
   ],
   "source": [
    "text_entry_example = train_lines[24][11:]\n",
    "print(text_entry_example)"
   ]
  },
  {
   "source": [
    "### WhitespaceTokenizer\n",
    "Using the ```nltk.tokenize.WhitespaceTokenizer```, we can separate the above review using white-space as the delimiter:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['i', 'liked', 'this', 'album', 'more', 'then', 'i', 'thought', 'i', 'would:', 'I', 'heard', 'a', 'song', 'or', 'two', 'and', 'thought', 'same', 'o', 'same', 'o,but', 'when', 'i', 'listened', 'to', 'songs', 'like', '\"blue', 'angel\",\"lanna\"', 'and', '\\'mama\"', 'the', 'hair', 'just', 'rose', 'off', 'my', 'neck.Roy', 'is', 'trully', 'an', 'amazing', 'singer', 'with', 'a', 'talent', 'you', \"don't\", 'find', 'much', 'now', 'days.']\n"
     ]
    }
   ],
   "source": [
    "white_space_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "print(white_space_tokenizer.tokenize(text_entry_example))"
   ]
  },
  {
   "source": [
    "The problem with this tokenization method is that tokens like 'would:', 'neck.Roy', '\"blue', 'angel\"', and 'days.' don't have much meaning. 'would:' and 'would' have different meanings. Further, 'neck.Roy' and 'neck', 'roy' also have different meanings. We need to somehow take into account the puntuation present in each review. Further, we need to factor in capital letters, plural and singular forms of words, and spelling mistakes. Consider another example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Don't try to fool us with fake reviews.: It's glaringly obvious that all of the glowing reviews have been written by the same person, perhaps the author herself. They all have the same misspellings and poor sentence structure that is featured in the book. Who made Veronica Haddon think she is an author?\n\n"
     ]
    }
   ],
   "source": [
    "text_entry_example = train_lines[15][11:]\n",
    "print(text_entry_example)"
   ]
  },
  {
   "source": [
    "### WordPunctTokenizer\n",
    "To take into account the punctuation present in a review, we will try the ```nltk.tokenize.WordPunctTokenizer```:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Don', \"'\", 't', 'try', 'to', 'fool', 'us', 'with', 'fake', 'reviews', '.:', 'It', \"'\", 's', 'glaringly', 'obvious', 'that', 'all', 'of', 'the', 'glowing', 'reviews', 'have', 'been', 'written', 'by', 'the', 'same', 'person', ',', 'perhaps', 'the', 'author', 'herself', '.', 'They', 'all', 'have', 'the', 'same', 'misspellings', 'and', 'poor', 'sentence', 'structure', 'that', 'is', 'featured', 'in', 'the', 'book', '.', 'Who', 'made', 'Veronica', 'Haddon', 'think', 'she', 'is', 'an', 'author', '?']\n"
     ]
    }
   ],
   "source": [
    "word_punct_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(text_entry_example))"
   ]
  },
  {
   "source": [
    "From the above output we can see tokens like \"t\", \"s\", and \"'\" are occurring. These tokens have no meaning. \n",
    "\n",
    "### TreebankWordTokenizer\n",
    "To solve this problem we can use a more advanced tokenization method that transforms these types of tokens into tokens that are more meaningful called the ```nltk.tokenize.TreebankWordTokenizer```:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Do', \"n't\", 'try', 'to', 'fool', 'us', 'with', 'fake', 'reviews.', ':', 'It', \"'s\", 'glaringly', 'obvious', 'that', 'all', 'of', 'the', 'glowing', 'reviews', 'have', 'been', 'written', 'by', 'the', 'same', 'person', ',', 'perhaps', 'the', 'author', 'herself.', 'They', 'all', 'have', 'the', 'same', 'misspellings', 'and', 'poor', 'sentence', 'structure', 'that', 'is', 'featured', 'in', 'the', 'book.', 'Who', 'made', 'Veronica', 'Haddon', 'think', 'she', 'is', 'an', 'author', '?']\n"
     ]
    }
   ],
   "source": [
    "tree_bank_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "text_entry_tb_output = tree_bank_tokenizer.tokenize(text_entry_example)\n",
    "print(text_entry_tb_output)"
   ]
  },
  {
   "source": [
    "\"Do\" + \"n't\" converys the meaning of \"Don't\" better than \"Don\", \"'\", \"t\". The ```TreebankWordTokenizer``` presents the most effective way to extract meaning from these sentences. \n",
    "\n",
    "## Token Normalization\n",
    "Now that we have split our data into tokens, we must further parse these tokens. It may be the case that we want the same token for different forms of a given word. For example, we may want both \"pen\" and \"pens\" to be represented by the \"pen\" token. Moreover, we may want \"person\", \"people\", and \"persons\" to all be represented by \"person\". There are two ways in which we can concatenate these tokens: stemming and lematization.\n",
    "\n",
    "### Stemming\n",
    "Stemming is the process of removing and/or replacing the suffixes of words to obtain the root meaning of the word. This normalization method simply cuts off the suffixes of various words to obtain simplified and understandable tokens. Let's apply the ```nltk.stem.PorterStemmer``` normalization method to a list of abnormal and plural words:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Word = persons, Stemmed Word = person\n\nOriginal Word = Feet, Stemmed Word = feet\n\nOriginal Word = apples, Stemmed Word = appl\n\nOriginal Word = trying, Stemmed Word = tri\n\nOriginal Word = fries, Stemmed Word = fri\n\nOriginal Word = geese, Stemmed Word = gees\n\nOriginal Word = women, Stemmed Word = women\n\n"
     ]
    }
   ],
   "source": [
    "pstemmer = nltk.stem.PorterStemmer()\n",
    "plural_words = [\"persons\", \"Feet\", \"apples\", \"Trying\", \"fries\", \"geese\", \"women\"]\n",
    "for word in plural_words:\n",
    "    print(\"Original Word = {}, Stemmed Word = {}\\n\".format(word, pstemmer.stem(word)))"
   ]
  },
  {
   "source": [
    "From the output above, we can see that the stemmer handles words like persons, apples, and fries correctly. Notice that the stemmer also modifies the casing of each word such that the words are lower case. The stemmer fails, however, to modify women, geese, and feet to their singular counterparts. The following normalization method addresses these situations. \n",
    "\n",
    "### Lemmatization\n",
    "A lemmatizer looks up the tokens using a database formed using vast amounts of text. This normalization technique can properly address abnormal plural words. The following is an implementation of the ```nltk.stem.WordNetLemmatizer```:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Original Word = persons, Stemmed Word = person\n",
      "\n",
      "Original Word = feet, Stemmed Word = foot\n",
      "\n",
      "Original Word = apples, Stemmed Word = apple\n",
      "\n",
      "Original Word = trying, Stemmed Word = trying\n",
      "\n",
      "Original Word = fries, Stemmed Word = fry\n",
      "\n",
      "Original Word = geese, Stemmed Word = goose\n",
      "\n",
      "Original Word = women, Stemmed Word = woman\n",
      "\n",
      "Original Word = good, Stemmed Word = good\n",
      "\n",
      "Original Word = better, Stemmed Word = better\n",
      "\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\purpl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "wnlemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "for word in plural_words:\n",
    "    print(\"Original Word = {}, Stemmed Word = {}\\n\".format(word, wnlemmatizer.lemmatize(word)))"
   ]
  },
  {
   "source": [
    "The problem with this normalization technique is that some words that have different meanings are reduced to the same lemma in niche circumstances. It is important to determine the context in which these tokenization methods are used so that we can make an informed decision as to which one will work better.\n",
    "\n",
    "# Token Representations\n",
    "Now that we have normalized the tokens appropriately, we must represent these tokens in a way that a machine can understand them. There are a few ways that we can do this. We can manually create representations for tokens using ordinal encodings, one-hot encodings, or by manually training a set of custom word embeddings. Alternatively, we can use pre-trained word embeddings like Word2Vec embeddings and GloVe embeddings.\n",
    "\n",
    "## Custom Representations\n",
    "\n",
    "\n",
    "## Pre-Trained Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}