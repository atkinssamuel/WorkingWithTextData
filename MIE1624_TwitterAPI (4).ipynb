{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Demonstrating Python modules and data structures that can be used to efficiently work with Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python modules can be used to efficiently work with Twitter data:\n",
    "\n",
    "tweepy (this module will be used in this notebook for our demonstration of the Twitter API for Python)\n",
    "\n",
    "Python Twitter Tools\n",
    "\n",
    "python-twitter\n",
    "\n",
    "twython\n",
    "\n",
    "TwitterAPI\n",
    "\n",
    "TwitterSearch\n",
    "\n",
    "Source: https://stackabuse.com/accessing-the-twitter-api-with-python/\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data structures used in the Python API for Twitter\n",
    "    \n",
    "Typically, Twitter data is pulled using the JSON data structure which you would have to parse either into csv or a pandas dataframe, depending on your purpose of the results.\n",
    "\n",
    "In the module used in this demonstration (Tweepy), tweets are pulled as tweepy objects. These objects are then converted into json so that we can parse through keys and values easier to gather tweet metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Using the Twitter API for Python to download tweets, search tweets by hashtags, extract metadata (i.e. number of reteweets, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary libraries used for Twitter API extraction and analysis are tweepy, csv, and json. Tweepy is the Twitter API library for Python, which is the most mature compared to all python libraries available for the Twitter API. The CSV library is used to save extracted tweets and underlying metadata into. The JSON library is used to parse and format tweet metadata into a format which is easy to manipulate because we can use dict keys and values to extract underlying metadata details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "import datetime\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State the keys to authenticate to the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to setup and be approved for a Developer account in order to receive these keys. These access keys are necessary in order to authenticate into the Twitter API using the tweepy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key= 'crdecmmwhUaTV7oitShaB7xlV'\n",
    "consumer_secret= 'pEE16H07j9ygOmaxPyJBlW9LUZIrkjOwSyBwhk3DWTS5yZKzEX'\n",
    "access_token= '1242649299978256389-Ba9M1Nudxuue16nFtGAXuzPk5NNnja'\n",
    "access_token_secret= 'Skqz04ZBTGAob4K61cHBSay3myFyGLJiCUFPjd7rxyEIk'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate to your Twitter App  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the access key values into the OAuth handler, which is a function of the tweepy library that allows us to authenticate given acceptable credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User(_api=<tweepy.api.API object at 0x000002128FC99CF8>, _json={'id': 1242649299978256389, 'id_str': '1242649299978256389', 'name': 'Rohith', 'screen_name': 'rohith_so', 'location': 'Toronto, Canada', 'profile_location': None, 'description': 'PhD researcher in Machine Learning @UofT and Cloud security Engineer @Deloitte', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': True, 'followers_count': 3, 'friends_count': 62, 'listed_count': 0, 'created_at': 'Wed Mar 25 03:07:47 +0000 2020', 'favourites_count': 5, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 0, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1257200110250921984/a2rubQEa_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1257200110250921984/a2rubQEa_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/1242649299978256389/1588575155', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none', 'suspended': False, 'needs_phone_verification': False}, id=1242649299978256389, id_str='1242649299978256389', name='Rohith', screen_name='rohith_so', location='Toronto, Canada', profile_location=None, description='PhD researcher in Machine Learning @UofT and Cloud security Engineer @Deloitte', url=None, entities={'description': {'urls': []}}, protected=True, followers_count=3, friends_count=62, listed_count=0, created_at=datetime.datetime(2020, 3, 25, 3, 7, 47), favourites_count=5, utc_offset=None, time_zone=None, geo_enabled=False, verified=False, statuses_count=0, lang=None, contributors_enabled=False, is_translator=False, is_translation_enabled=False, profile_background_color='F5F8FA', profile_background_image_url=None, profile_background_image_url_https=None, profile_background_tile=False, profile_image_url='http://pbs.twimg.com/profile_images/1257200110250921984/a2rubQEa_normal.jpg', profile_image_url_https='https://pbs.twimg.com/profile_images/1257200110250921984/a2rubQEa_normal.jpg', profile_banner_url='https://pbs.twimg.com/profile_banners/1242649299978256389/1588575155', profile_link_color='1DA1F2', profile_sidebar_border_color='C0DEED', profile_sidebar_fill_color='DDEEF6', profile_text_color='333333', profile_use_background_image=True, has_extended_profile=False, default_profile=True, default_profile_image=False, following=False, follow_request_sent=False, notifications=False, translator_type='none', suspended=False, needs_phone_verification=False)\n"
     ]
    }
   ],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "     \n",
    "users = api.me()\n",
    "print(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search through tweets by hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We search for the hashtag #wildfires which we define as a search term. We then use Tweepy's Cursor function to pass this search term into Tweepy's api.search function which allows us to conduct queries on available public tweets from a specified date, which is defined by the variable \"date_since\". We defined this variable to extract data from November 16th 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tweepy.cursor.ItemIterator object at 0x000002128FCDB6A0>\n",
      "1323291868550078464 RT @i_ameztoy: Do you want a scary #Halloween? Here you go two months of CO evolution; Look at the tongues crossing oceans! üßê\n",
      "\n",
      "@CopernicusE‚Ä¶\n",
      "1323291275320205312 #Wine country, fire country https://t.co/wtvGt9980N from @sfchronicle #winecountry #wildfires\n",
      "1323290275586936832 RT @Alex_Bernhardt: A key reason for the increase in #wildfires is forest management ‚Äî is the solution biomass? Learn more from Stan Parton‚Ä¶\n",
      "1323287849899225093 RT @ClimateSignals: Climate change is causing bigger, more frequent #wildfires to burn hotter and spread faster. Scientists have identified‚Ä¶\n",
      "1323287814193238016 RT @MarineGOfficial: #SavePantanal: The Pantanal is a terrestrial ecoregion of South America belonging to the prairie and flooded savannah‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "# Define the search term and the date_since date as variables\n",
    "search_hashtag = \"#wildfires\"\n",
    "date_since = \"2018-11-16\"\n",
    "\n",
    "# Collect tweets\n",
    "tweets = tw.Cursor(api.search,\n",
    "              q=search_hashtag,\n",
    "              lang=\"en\",\n",
    "              since=date_since).items(5)\n",
    "print(tweets)\n",
    "\n",
    "# Iterate and print tweets\n",
    "for tweet in tweets:\n",
    "    print(tweet.id, tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract metadata (i.e. number of retweets etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function api.get_status to pull the full text of a retweeted status given a tweet ID then convert this object into JSON format in order to manipulate the underlying metadata elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a coding fanatic who wants to work with us and learn new technologies? üë®‚Äçüíªüë©‚Äçüíª\n",
      "Well then, we are looking just for you!\n",
      "\n",
      "Register for our SDE Hiring Challenge right now!\n",
      "https://t.co/Zg08gHhT0W  \n",
      "\n",
      "#hiring #challenge #coding #programming https://t.co/1N7gXaH9eA\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Thu May 28 06:14:48 +0000 2020',\n",
       " 'id': 1265889240300257280,\n",
       " 'id_str': '1265889240300257280',\n",
       " 'full_text': 'Are you a coding fanatic who wants to work with us and learn new technologies? üë®\\u200düíªüë©\\u200düíª\\nWell then, we are looking just for you!\\n\\nRegister for our SDE Hiring Challenge right now!\\nhttps://t.co/Zg08gHhT0W  \\n\\n#hiring #challenge #coding #programming https://t.co/1N7gXaH9eA',\n",
       " 'truncated': False,\n",
       " 'display_text_range': [0, 242],\n",
       " 'entities': {'hashtags': [{'text': 'hiring', 'indices': [203, 210]},\n",
       "   {'text': 'challenge', 'indices': [211, 221]},\n",
       "   {'text': 'coding', 'indices': [222, 229]},\n",
       "   {'text': 'programming', 'indices': [230, 242]}],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [],\n",
       "  'urls': [{'url': 'https://t.co/Zg08gHhT0W',\n",
       "    'expanded_url': 'https://practice.geeksforgeeks.org/contest/hiring-challenge-sde',\n",
       "    'display_url': 'practice.geeksforgeeks.org/contest/hiring‚Ä¶',\n",
       "    'indices': [176, 199]}],\n",
       "  'media': [{'id': 1265887151016812546,\n",
       "    'id_str': '1265887151016812546',\n",
       "    'indices': [243, 266],\n",
       "    'media_url': 'http://pbs.twimg.com/media/EZFVqCoWoAILfq5.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/EZFVqCoWoAILfq5.jpg',\n",
       "    'url': 'https://t.co/1N7gXaH9eA',\n",
       "    'display_url': 'pic.twitter.com/1N7gXaH9eA',\n",
       "    'expanded_url': 'https://twitter.com/geeksforgeeks/status/1265889240300257280/photo/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'medium': {'w': 1200, 'h': 1200, 'resize': 'fit'},\n",
       "     'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1200, 'h': 1200, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 680, 'resize': 'fit'}}}]},\n",
       " 'extended_entities': {'media': [{'id': 1265887151016812546,\n",
       "    'id_str': '1265887151016812546',\n",
       "    'indices': [243, 266],\n",
       "    'media_url': 'http://pbs.twimg.com/media/EZFVqCoWoAILfq5.jpg',\n",
       "    'media_url_https': 'https://pbs.twimg.com/media/EZFVqCoWoAILfq5.jpg',\n",
       "    'url': 'https://t.co/1N7gXaH9eA',\n",
       "    'display_url': 'pic.twitter.com/1N7gXaH9eA',\n",
       "    'expanded_url': 'https://twitter.com/geeksforgeeks/status/1265889240300257280/photo/1',\n",
       "    'type': 'photo',\n",
       "    'sizes': {'medium': {'w': 1200, 'h': 1200, 'resize': 'fit'},\n",
       "     'thumb': {'w': 150, 'h': 150, 'resize': 'crop'},\n",
       "     'large': {'w': 1200, 'h': 1200, 'resize': 'fit'},\n",
       "     'small': {'w': 680, 'h': 680, 'resize': 'fit'}}}]},\n",
       " 'source': '<a href=\"https://mobile.twitter.com\" rel=\"nofollow\">Twitter Web App</a>',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': None,\n",
       " 'in_reply_to_user_id_str': None,\n",
       " 'in_reply_to_screen_name': None,\n",
       " 'user': {'id': 57741058,\n",
       "  'id_str': '57741058',\n",
       "  'name': 'GeeksforGeeks',\n",
       "  'screen_name': 'geeksforgeeks',\n",
       "  'location': 'India',\n",
       "  'description': 'üë®üèª\\u200düíªüßëüèº\\u200düíªüë©üèª\\u200düíª - üëçüèª ; üêùüêûüêõüêåüêúüï∑ü¶óü¶ü - ‚ùå',\n",
       "  'url': 'https://t.co/1Dm8vpxhFQ',\n",
       "  'entities': {'url': {'urls': [{'url': 'https://t.co/1Dm8vpxhFQ',\n",
       "      'expanded_url': 'http://geeksforgeeks.org',\n",
       "      'display_url': 'geeksforgeeks.org',\n",
       "      'indices': [0, 23]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 20776,\n",
       "  'friends_count': 22,\n",
       "  'listed_count': 156,\n",
       "  'created_at': 'Fri Jul 17 20:02:09 +0000 2009',\n",
       "  'favourites_count': 712,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': False,\n",
       "  'statuses_count': 13900,\n",
       "  'lang': None,\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'FFFDF7',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme13/bg.gif',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme13/bg.gif',\n",
       "  'profile_background_tile': False,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1304985167476523008/QNHrwL2q_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1304985167476523008/QNHrwL2q_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/57741058/1603173723',\n",
       "  'profile_link_color': '119E39',\n",
       "  'profile_sidebar_border_color': 'D3D2CF',\n",
       "  'profile_sidebar_fill_color': 'E3E2DE',\n",
       "  'profile_text_color': '0D0C0C',\n",
       "  'profile_use_background_image': False,\n",
       "  'has_extended_profile': False,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'none'},\n",
       " 'geo': None,\n",
       " 'coordinates': None,\n",
       " 'place': None,\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 7,\n",
       " 'favorite_count': 19,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'possibly_sensitive': False,\n",
       " 'possibly_sensitive_appealable': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting the full text of a retweeted status of a given tweet ID by first checking if the tweet has been retweeted\n",
    "\n",
    "id = \"1265889240300257280\"\n",
    "status = api.get_status(id, tweet_mode=\"extended\")\n",
    "try:\n",
    "    print(status.retweeted_status.full_text)\n",
    "except AttributeError: # Not a Retweet\n",
    "    print(status.full_text)\n",
    "\n",
    "#Convert the tweet status into JSON so we can parse the dict keys and gather underlying metadata\n",
    "json_str = json.dumps(status._json)\n",
    "metadata = (json.loads(json_str))\n",
    "metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all elements of the metadata variable (JSON format of the retweeted status object) can be seen in a clean JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the metadata variable is now in JSON format, we can view the keys of the variable since it is a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['created_at', 'id', 'id_str', 'full_text', 'truncated', 'display_text_range', 'entities', 'extended_entities', 'source', 'in_reply_to_status_id', 'in_reply_to_status_id_str', 'in_reply_to_user_id', 'in_reply_to_user_id_str', 'in_reply_to_screen_name', 'user', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status', 'retweet_count', 'favorite_count', 'favorited', 'retweeted', 'possibly_sensitive', 'possibly_sensitive_appealable', 'lang'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gather the keys of the tweet's metadata\n",
    "metadata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these key values, we can now easily identify how we want to extract underlying metadata elements by searching through the keys of the metadata dict variable. For example, as seen below we can get the user metadata information by analyzing the name key within the user key.\n",
    "\n",
    "We now use this to gather when the tweet was published, by which user, from wht country, and how many followeres and friends the user has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet was created at Thu May 28 06:14:48 +0000 2020 by the user GeeksforGeeks from India \n",
      "This user has 20776 followers and 22 friends\n"
     ]
    }
   ],
   "source": [
    "#Gather the user of the tweet\n",
    "user = metadata['user']['name']\n",
    "#Gather the location of a user's tweet\n",
    "user_location = metadata['user']['location']\n",
    "\n",
    "#Gather the time the tweet was made\n",
    "created_at = metadata['created_at']\n",
    "\n",
    "#Gather details about the user's followers and friends\n",
    "number_of_followers = metadata['user']['followers_count']\n",
    "number_of_friends = metadata['user']['friends_count']\n",
    "\n",
    "print(\"The tweet was created at\",created_at,\"by the user\",user,\"from\",user_location,\"\\nThis user has\",number_of_followers,\"followers and\",number_of_friends,\"friends\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harshitabambure\n",
      "codedailybot\n",
      "UVahalkar\n",
      "codedailybot\n",
      "ProjectLearn_io\n",
      "codedailybot\n",
      "AaronCuddeback\n"
     ]
    }
   ],
   "source": [
    "# printing the screen names of the retweeters of the given tweet id\n",
    "for retweet in retweets_list: \n",
    "    print(retweet.user.screen_name) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the Tweet ID, there were: 7 retweets found\n"
     ]
    }
   ],
   "source": [
    "#printing the number of retweets for a tweet \n",
    "retweets_list = api.retweets(id) \n",
    "\n",
    "number_of_retweets = len(retweets_list)\n",
    "print(\"\\nBased on the Tweet ID, there were:\", number_of_retweets, \"retweets found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Using the Twitter API to download tweets and save those as a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we search for the last 100 tweets made using the hashtag \"#trump\" and save these tweets to a csv along with metadata of those tweets: username/screen name, id of the tweet, whether it was retweeted, language of the tweet, number of followers of the user, whether the user is verified, location the tweet was made in, the tweet, and when it was created. Our search results are then saved to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets have been saved to the following csv file:hashtag_tweets.csv\n"
     ]
    }
   ],
   "source": [
    "#method to get a user's last tweets\n",
    "def get_tweets(hashtag):\n",
    "\n",
    " \n",
    "\n",
    "    #set count to however many tweets you want\n",
    "    number_of_tweets = 100\n",
    "\n",
    "    #get tweets\n",
    "    tweets_for_csv = []\n",
    "    for tweet in tweepy.Cursor(api.search, q = hashtag).items(number_of_tweets):\n",
    "        #create array of tweet information: username, tweet id, date/time, text\n",
    "        tweets_for_csv.append([tweet.user.screen_name,tweet.retweeted,tweet.user.lang,tweet.user.followers_count,tweet.user.verified,tweet.user.location.encode(\"utf-8\"),tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\")])\n",
    "\n",
    "    #write to a new csv file from the array of tweets\n",
    "    outfile = \"hashtag_tweets.csv\"\n",
    "    print (\"tweets have been saved to the following csv file:\" + outfile)\n",
    "    with open(outfile, 'w+') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerows(tweets_for_csv)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_tweets(\"#trump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Basic feature extraction and basic text preprocessing on tweets from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
